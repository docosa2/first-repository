{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84920179",
   "metadata": {},
   "source": [
    "# GD-14 BERT Pretrained Model\n",
    "\n",
    "### 2023-02-14 (화)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8395b0fc",
   "metadata": {},
   "source": [
    "## 라이브러리 버전을 확인해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1891fdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.4\n",
      "1.3.3\n",
      "3.4.3\n",
      "2.0.9\n",
      "2.2.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import numpy\n",
    "import pandas\n",
    "import matplotlib\n",
    "import json\n",
    "import re\n",
    "\n",
    "print(tensorflow.__version__)\n",
    "print(numpy.__version__)\n",
    "print(pandas.__version__)\n",
    "print(matplotlib.__version__)\n",
    "print(json.__version__)\n",
    "print(re.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b55cd1c",
   "metadata": {},
   "source": [
    "## TO DO\n",
    "1. Tokenizer 준비\n",
    "\n",
    "SentencePiece 모델을 이용해 BERT의 MLM 학습용 데이터를 만드세요.\n",
    "\n",
    "이를 위해 한글 나무 위키 코퍼스로부터 8000의 vocab_size를 갖는 sentencepiece 모델을 만들어 보세요. BERT에 사용되는 주요 특수문자가 vocab에 포함되어야 합니다. (시간이 부족하다면 클라우드에 저장된 sentencepiece 모델을 사용하세요.)\n",
    "\n",
    "2. 데이터 전처리 (1) MASK 생성\n",
    "\n",
    "BERT의 MLM에 필요한 빈칸(mask)을 학습 데이터 전체 토큰의 15% 정도로 만들어 주세요. 그 중 80%는 [MASK] 토큰, 10%는 랜덤한 토큰, 나머지 10%는 원래의 토큰을 그대로 사용하세요.\n",
    "\n",
    "3. 데이터 전처리 (2) NSP pair 생성\n",
    "\n",
    "BERT의 pretrain task인 NSP는 두 문장이 연속하는지 확인하는 것입니다. 이를 위해 2개의 문장을 짝지어 50%의 확률로 TRUE와 FALSE를 지정해 주세요.\n",
    "\n",
    "두 문장 사이에 segment 처리를 해주세요. 첫 번째 문장의 segment는 0, 두 번째 문장은 1로 채워준 후 둘 사이에 구분자인 [SEP] 등을 넣어주세요.\n",
    "\n",
    "MLM과 NSP는 동시에 학습된다는 것을 염두에 두고 학습 데이터를 구성해 보세요.\n",
    "\n",
    "4. 데이터 전처리 (3) 데이터셋 완성\n",
    "\n",
    "BERT pretrain 데이터셋을 생성해, json 포맷으로 저장하세요. 데이터셋의 사이즈가 크므로np.memmap을 사용해 메모리 사용량을 최소화해 보세요.\n",
    "\n",
    "5. BERT 모델 구현\n",
    "\n",
    "pad mask, ahead mask 함수, gelu activation 함수, parameter initializer 생성 함수, json을 config 형태로 사용하기 위한 유틸리티 함수를 먼저 만들어 두세요.\n",
    "\n",
    "Embedding 레이어, Transformer encoder 레이어, BERT 레이어를 구성한 후, pretraine용 BERT 모델을 만들어 봅시다.\n",
    "\n",
    "6. pretrain 진행\n",
    "\n",
    "loss, accuracy 함수를 정의하고 Learning Rate 스케쥴링을 구현한 후, 10 Epoch까지 모델 학습을 시켜보세요. 학습을 진행할 때는 배치 사이즈에 유의하세요.\n",
    "\n",
    "7. 프로젝트 결과\n",
    "\n",
    "학습된 모델과 학습과정을 시각화해 보세요. NSP와 MLM의 loss가 안정적으로 수렴하나요? 모델이 작기 때문에 loss가 잘 수렴하지 않을 수도 있어요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62bf807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff56570b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
