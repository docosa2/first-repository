{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e66b24",
   "metadata": {},
   "source": [
    "# NLP GD-10 Transformer Translator\n",
    "\n",
    "### 2023-01-30 (월)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f042dc8",
   "metadata": {},
   "source": [
    "### 한글 폰트 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "41bb42ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager.findfont(font)\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf5d2c2",
   "metadata": {},
   "source": [
    "## 라이브러리 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "04d6c4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "import seaborn # Attention 시각화를 위해 필요!\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e64858",
   "metadata": {},
   "source": [
    "## 트랜스포머 내부 모듈 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3db3b2",
   "metadata": {},
   "source": [
    "### 1) Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "361d1954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    return sinusoid_table\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a026181",
   "metadata": {},
   "source": [
    "### 2) Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "31b382eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model                     # 512\n",
    "        \n",
    "        self.depth = d_model // self.num_heads     # 512/8 = 64\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model) # Linear Layer\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "\n",
    "        \"\"\"\n",
    "        Scaled QK 값 구하기\n",
    "        \"\"\"\n",
    "        dk = tf.cast(K.shape[-1], tf.float32) # K의 차원수를 float32형으로 변환\n",
    "        QK = tf.matmul(Q, K, trainspose_b=True) # QKT\n",
    "        \n",
    "        scaled_qk = QK/tf.math.sqrt(dk)\n",
    "        \n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9) \n",
    "\n",
    "        \"\"\"\n",
    "        1. Attention Weights 값 구하기 -> attentions\n",
    "        2. Attention 값을 V에 곱하기 -> out\n",
    "        \"\"\" \n",
    "        atteentions = tf.nn.softmax(scaled_qk, axis=-1) # 계산된 QK^T/sqrt(dK)값을 소프트맥스하여 확률 값으로 변환한 것이 어텐션\n",
    "        out = tf.matmul(attentions, V) # 어텐션과 밸류(V)의 내적이 출력값\n",
    "        \n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Embedding을 Head의 수로 분할하는 함수\n",
    "\n",
    "        x: [ batch x length x emb ] # [64, 40, 512]\n",
    "        return: [ batch x length x heads x self.depth ] \n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]   # 64                  # 512//8 == 64\n",
    "        split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth)) # (64,-1, 8, 64)\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3]) # (64, 8, [40], 64) \n",
    "        # (batch_size, num_heads, length, depth)\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        분할된 Embedding을 하나로 결합하는 함수\n",
    "                                                    # (batch_size, num_heads, length, depth)\n",
    "        x: [ batch x length x heads x self.depth ]  # [64, 8, 40, 64]\n",
    "        return: [ batch x length x emb ]            # [64, 40, -1(512)] \n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        # combined_x = tf.transpose(x, perm[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (batch_size, -1, self.d_model))\n",
    "        \n",
    "        return combined_x\n",
    "    \n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        \"\"\"\n",
    "        아래 순서에 따라 소스를 작성하세요.\n",
    "\n",
    "        Step 1: Linear_in(Q, K, V) -> WQ, WK, WV\n",
    "        Step 2: Split Heads(WQ, WK, WV) -> WQ_split, WK_split, WV_split\n",
    "        Step 3: Scaled Dot Product Attention(WQ_split, WK_split, WV_split)\n",
    "                 -> out, attention_weights\n",
    "        Step 4: Combine Heads(out) -> out\n",
    "        Step 5: Linear_out(out) -> out\n",
    "        \"\"\"\n",
    "        # Step 1: Linear_in(Q, K, V) -> WQ, WK, WV\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        # Step 2: Split Heads(WQ, WK, WV) -> WQ_split, WK_split, WV_split\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "           \n",
    "        # Step 3: Scaled Dot Product Attention(WQ_split, WK_split, WV_split)\n",
    "        #         -> out, attention_weights                \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "        \n",
    "        # Step 4: Combine Heads(out) -> out\n",
    "        out = self.combine_heads(out)\n",
    "        \n",
    "        # Step 5: Linear_out(out) -> out\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out, attention_weights\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd2757b",
   "metadata": {},
   "source": [
    "### Position-wise Feed-Forward Network\n",
    "\n",
    "* d_ff : 논문의 설명대로라면 2048  \n",
    "* d_model : 512\n",
    "\n",
    "\n",
    "* [ batch x length x d_model ]의 입력을 받아\n",
    "\n",
    "-> w_1 이 2048차원으로 매핑  \n",
    "-> 활성함수 ReLU를 적용  \n",
    "\n",
    "-> 다시 w_2 를 통해 512차원으로 되돌리는 과정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "ccce8f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):                            # 512, 2048\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu') # 2048\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)                 # 512\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "            \n",
    "        return out\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709d95eb",
   "metadata": {},
   "source": [
    "## 트랜스포머 모듈 조립하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3ec4cf",
   "metadata": {},
   "source": [
    "### 레이어 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce103f57",
   "metadata": {},
   "source": [
    "#### Encoder 레이어 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4768eb",
   "metadata": {},
   "source": [
    "Transformer의 구현은 정말 많은데, 그중에서 Normalization Layer의 위치에 대한 논의가 종종 나온답니다. 실제 논문에서는 [Input] - [Module] - [Residual] - [Norm] (Module = MHA, FFN)으로 표현되어 있지만 정작 Official 구현인 구글의 Tensor2Tensor 에서는 [Input] - [Norm] - [Module] - [Residual] 방식을 사용했어요.\n",
    "\n",
    "필자의 경험에 따르면 레이어가 많아질수록 후자가 약간 더 좋은 성능을 보였기에 필자는 논문 대신 Official 구현을 따르길 권장합니다! 이번 프로젝트는 소규모라서 큰 차이가 나지 않으니 알아두기만 해도 괜찮아요. 😃\n",
    "\n",
    "트랜스포머의 Layer Normalization의 위치에 대한 논의를 다룬 [On Layer Normalization in the Transformer Architecture](https://arxiv.org/pdf/2002.04745.pdf)이라는 제목의 논문이 2020년 초반에 발표되었습니다. 이 논문에서는 모듈 앞에 Normalization Layer를 두는 pre-LN 방식이 왜 유리한지를 설명하고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "4c170d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):                # 512, 8, 2048, 0.0\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)       # (512, 8)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)                 # (512, 2048)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)                 # 0.0\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x                                                    # for residual connection\n",
    "        out = self.norm_1(x)                                            # layer normalization\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)         # multi head attention \n",
    "        out = self.dropout(out)                                         # dropout\n",
    "        out += residual                                                 # residual connection\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out                                                  # for residual connection\n",
    "        out = self.norm_2(out)                                          # layer normalization\n",
    "        out = self.ffn(out)                                             # Position-wise Feed forward network\n",
    "        out = self.dropout(out)                                         # dropout\n",
    "        out += residual                                                 # residual connection\n",
    "        \n",
    "        return out, enc_attn\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfe6f39",
   "metadata": {},
   "source": [
    "#### 디코더 레이어 구현\n",
    "\n",
    "위 EncoderLayer 클래스를 참고하여 DecoderLayer 클래스를 완성하세요!  \n",
    "(참고: Decoder에서는 두 번의 Attention이 진행되니 반환되는 Attention도 두 개겠죠?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6843cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):               # 512, 8, 2048, 0.0\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)      # (512, 8)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)       # (512, 8)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)                  # (512, 2048)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)   \n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        \n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f859921e",
   "metadata": {},
   "source": [
    "### 인코더-디코더 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2e8455",
   "metadata": {},
   "source": [
    "#### 인코더 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "eb958a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,        # 인코더 레이어의 층 수 6\n",
    "                 d_model,         # 512\n",
    "                 n_heads,         # 8\n",
    "                 d_ff,            # 2048 \n",
    "                 dropout):        # 0.1\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.n_layers = n_layers  # 8\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) # (512, 8, 2048, 0.0)\n",
    "                        for _ in range(n_layers)]                        # 6\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):                                   # 6\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)                #\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5b6991",
   "metadata": {},
   "source": [
    "#### 디코더 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "1d8f3e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc8631d",
   "metadata": {},
   "source": [
    "## Transformer 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea83578",
   "metadata": {},
   "source": [
    "정의된 Encoder 와 Decoder 를 가지고 최종적으로 트랜스포머를 완성합니다!\n",
    "\n",
    "아래 조건을 만족하며 소스의 빈칸을 채워 Transformer 클래스를 완성하세요!\n",
    "\n",
    "1. shared 변수를 매개변수로 받아 True 일 경우 Decoder Embedding과 출력층 Linear의 Weight를 공유 할 수 있게 하세요! Weight가 공유될 경우 Embedding 값에 sqrt(d_model) 을 곱해줘야 하는 것, 잊지 않으셨죠? (참고: tf.keras.layers.Layer.set_weights())\n",
    "\n",
    "\n",
    "2. 우리가 정의한 positional_encoding 의 반환값 형태는 [ Length x d_model ] 인데, 이를 더해 줄 Embedding 값 형태가 [ Batch x Length x d_model ] 이라서 연산이 불가능합니다. 연산이 가능하도록 수정하세요! (참고: tf.expand_dims(), np.newaxis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "83e9e904",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,         # 6\n",
    "                    d_model,          # 512\n",
    "                    n_heads,          # 8 \n",
    "                    d_ff,             # 2048\n",
    "                    src_vocab_size,   # 30000\n",
    "                    tgt_vocab_size,   # 30000\n",
    "                    pos_len,          # \n",
    "                    dropout=0.1,\n",
    "                    shared=True):        \n",
    "        \n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        \"\"\"\n",
    "        1. Embedding Layer 정의\n",
    "        2. Positional Encoding 정의\n",
    "        3. Encoder / Decoder 정의\n",
    "        4. Output Linear 정의\n",
    "        5. Shared Weights\n",
    "        6. Dropout 정의\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Embedding Layer 정의\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model) # (30000, 512)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model) # (30000, 512)\n",
    "\n",
    "        # 2. Positional Encoding 정의\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)         # (1024, 512)\n",
    "               \n",
    "        # 3. Encoder / Decoder 정의\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout) # (6, 512, 8, 0.1)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout) # (6, 512, 8, 0.1)\n",
    "        \n",
    "        # 4. Output Linear 정의\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)                   # (30000)\n",
    "\n",
    "        # 5. Shared Weights\n",
    "        self.shared = shared\n",
    "        if shared: \n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "        \n",
    "        #6. Dropout 정의\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)                   # 0.1\n",
    "        \n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1061b36d",
   "metadata": {},
   "source": [
    "## 모델 밖의 조력자들"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cb4294",
   "metadata": {},
   "source": [
    "#### Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50e9559",
   "metadata": {},
   "source": [
    "아까부터 은근하게 마음 한구석을 차지하고 있던 Masking을 살펴볼 시간이 다가왔습니다. 그리고 트랜스포머의 Learning Rate가 일반적이지 않다는 것도 기억하고 계실 거예요! 지금부터는 모델 외적인 부분을 정의해 주도록 하겠습니다. 이번 스텝에서는 데이터의 특성이나 학습 과정에 따라 달라지는 부분을 다루게 됩니다.\n",
    "\n",
    "먼저 Masking입니다. 이전 노드에서 배운 generate_causality_mask() 를 그대로 사용하면 되는데, 약간 추가할 내용이 있습니다! 아래 구현을 먼저 보실까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "8ab542cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c6ae02",
   "metadata": {},
   "source": [
    "generate_padding_mask() 는 Attention을 할 때에 <PAD> 토큰에도 Attention을 주는 것을 방지해 주는 역할을 합니다. 일전에 Sequence-to-Sequence 모델에서 Loss에 대한 Masking을 해줄 때도 위와 같은 방법으로 진행했죠? 한 배치의 데이터에서 <PAD> 토큰으로 이뤄진 부분을 모두 찾아내는 마스크를 생성합니다. 눈으로 직접 확인해보죠!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "7c1e44fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 25)\n",
      "(16, 35)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 700x700 with 3 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 288,
       "width": 594
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch, length = 16, 20\n",
    "src_padding = 5\n",
    "tgt_padding = 15\n",
    "\n",
    "src_pad = tf.zeros(shape=(batch, src_padding)) # (16, 5)\n",
    "tgt_pad = tf.zeros(shape=(batch, tgt_padding)) # (16, 15)\n",
    "\n",
    "sample_data = tf.ones(shape=(batch, length))   # (16, 20)\n",
    "\n",
    "sample_src = tf.concat([sample_data, src_pad], axis=-1) # [16, 20], [16,5]\n",
    "sample_tgt = tf.concat([sample_data, tgt_pad], axis=-1) # [16, 20], [16,15]\n",
    "\n",
    "print(sample_src.shape)\n",
    "print(sample_tgt.shape)\n",
    "\n",
    "enc_mask, dec_enc_mask, dec_mask = \\\n",
    "generate_masks(sample_src, sample_tgt) # [16, 20+5] [16, 20+15]\n",
    "\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax3 = fig.add_subplot(133)\n",
    "\n",
    "ax1.set_title('1) Encoder Mask')\n",
    "ax2.set_title('2) Encoder-Decoder Mask')\n",
    "ax3.set_title('3) Decoder Mask')\n",
    "\n",
    "ax1.imshow(enc_mask[:3, 0, 0].numpy(), cmap='Dark2')\n",
    "ax2.imshow(dec_enc_mask[0, 0].numpy(), cmap='Dark2')\n",
    "ax3.imshow(dec_mask[0, 0].numpy(), cmap='Dark2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d40fed8",
   "metadata": {},
   "source": [
    "첫 번째 마스크는 각 배치 별로 데이터의 꼬리 부분을 Masking 하는 형태임을 알 수 있습니다. 낯선 부분은 두 번째와 세 번째의 Decoder가 연관된 마스크인데... 이것이 바로 Causality Mask와 Padding Mask를 결합한 형태입니다! 자기 회귀적인 특성을 살리기 위해 Masked Multi-Head Attention에서 인과 관계 마스킹을 했던 것을 기억하시죠? 인과 관계를 가리는 것도 중요하지만 Decoder 역시 <PAD> 토큰은 피해 가야 하기 때문에 이런 형태의 마스크가 사용된답니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3c0b79",
   "metadata": {},
   "source": [
    "#### LR Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7325dcd0",
   "metadata": {},
   "source": [
    "또, 트랜스포머는 고정된 Learning Rate를 사용하지 않았었죠! 논문의 해당 부분을 Optimizer까지 포함하여 다시 한번 살펴봅시다. 이전 노드에서 Learning Rate를 numpy 로 간단히 구현을 했었는데, 이번엔 Tensorflow 상에서 잘 구동될 수 있도록 LearningRateSchedule 클래스를 상속받아 구현해보죠!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "8328a484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e574a4d5",
   "metadata": {},
   "source": [
    "트랜스포머가 제안한 수식이 아니더라도 가변적인 Learning Rate를 사용하려면 위와 같이 구현을 하시면 됩니다. Optimizer와 Scheduler를 연결하는 과정도 아주 간단하죠! Optimizer는 논문에 정의된 대로 Adam Optimizer를 사용하며 세부 파라미터도 동일하게 맞춰 줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbd44ca",
   "metadata": {},
   "source": [
    "# 프로젝트 : 더 멋진 번역기 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3d0d26",
   "metadata": {},
   "source": [
    "### 라이브러리 버전을 확인해 봅니다.\n",
    "---\n",
    "사용할 라이브러리 버전을 둘러봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "66b9f1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n",
      "1.22.4\n",
      "3.5.3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import numpy\n",
    "import matplotlib\n",
    "\n",
    "print(tensorflow.__version__)\n",
    "print(numpy.__version__)\n",
    "print(matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8360f1bc",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 다운로드 (클라우드 유저용)\n",
    "---\n",
    "아래 링크에서 korean-english-park.train.tar.gz 를 사용할 예정입니다. 다운로드할 필요는 없습니다.\n",
    "\n",
    "* [jungyeul/korean-parallel-corpora](https://github.com/jungyeul/korean-parallel-corpora/tree/master/korean-english-news-v1)\n",
    "\n",
    "☁️클라우드 환경에서는 위 데이터를 미리 준비해 놓았으니 연결만 시켜줍시다. 우측 하단의 Cloud shell을 열어주세요.\n",
    "아래와 같이 공유 디렉토리에 저장된 데이터를 가리키는 심볼릭 링크를 생성해 주시면 됩니다.\n",
    "\n",
    "$ ln -s ~/data ~/aiffel/transformer/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c82c91f",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 정제 및 토큰화\n",
    "---\n",
    "### 중복 데이타 제거 : Set\n",
    "\n",
    "1. set 데이터형이 중복을 허용하지 않는다는 것을 활용해 중복된 데이터를 제거하도록 합니다. 데이터의 병렬 쌍이 흐트러지지 않게 주의하세요! 중복을 제거한 데이터를 cleaned_corpus 에 저장합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d2ef3f",
   "metadata": {},
   "source": [
    "#### 데이타 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "bd473dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/nlp/transformer/data'\n",
    "kor_path = data_dir+\"/korean-english-park.train.ko\"\n",
    "eng_path = data_dir+\"/korean-english-park.train.en\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14c5f94",
   "metadata": {},
   "source": [
    "#### 데이타 살펴보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831c114f",
   "metadata": {},
   "source": [
    "##### 한글 데이타"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "7ace3b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 94123\n",
      "Example:\n",
      ">> 개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\n",
      ">> 북한의 핵무기 계획을 포기하도록 하려는 압력이 거세지고 있는 가운데, 일본과 북한의 외교관들이 외교 관계를 정상화하려는 회담을 재개했다.\n",
      ">> \"경호 로보트가 침입자나 화재를 탐지하기 위해서 개인적으로, 그리고 전문적으로 사용되고 있습니다.\"\n",
      ">> 수자원부 당국은 논란이 되고 있고, 막대한 비용이 드는 이 사업에 대해 내년에 건설을 시작할 계획이다.\n",
      ">> 또한 근력 운동은 활발하게 걷는 것이나 최소한 20분 동안 뛰는 것과 같은 유산소 활동에서 얻는 운동 효과를 심장과 폐에 주지 않기 때문에, 연구학자들은 근력 운동이 심장에 큰 영향을 미치는지 여부에 대해 논쟁을 해왔다.\n"
     ]
    }
   ],
   "source": [
    "with open(kor_path, \"r\") as f:\n",
    "    ko_raw = f.read().splitlines()\n",
    "\n",
    "print(\"Data Size:\", len(ko_raw))\n",
    "print(\"Example:\")\n",
    "\n",
    "for sen in ko_raw[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca05fdc",
   "metadata": {},
   "source": [
    "##### 영어 데이타"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "6a009f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 94123\n",
      "Example:\n",
      ">> Much of personal computing is about \"can you top this?\"\n",
      ">> Amid mounting pressure on North Korea to abandon its nuclear weapons program Japanese and North Korean diplomats have resumed talks on normalizing diplomatic relations.\n",
      ">> “Guard robots are used privately and professionally to detect intruders or fire,” Karlsson said.\n",
      ">> Authorities from the Water Resources Ministry plan to begin construction next year on the controversial and hugely expensive project.\n",
      ">> Researchers also have debated whether weight-training has a big impact on the heart, since it does not give the heart and lungs the kind of workout they get from aerobic activities such as brisk walking or running for at least 20 minutes.\n"
     ]
    }
   ],
   "source": [
    "with open(eng_path, \"r\") as f:\n",
    "    eng_raw = f.read().splitlines() # line 별로 split\n",
    "\n",
    "print(\"Data Size:\", len(eng_raw))\n",
    "print(\"Example:\")\n",
    "\n",
    "for sen in eng_raw[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "17059c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 정제 및 토큰화\n",
    "def clean_corpus(kor_path, eng_path):\n",
    "    with open(kor_path, \"r\") as f: kor = f.read().splitlines()\n",
    "    with open(eng_path, \"r\") as f: eng = f.read().splitlines()\n",
    "    assert len(kor) == len(eng)\n",
    "\n",
    "    # [[YOUR CODE]]\n",
    "    cleaned_corpus = [] # concatenate both corpus into one\n",
    "\n",
    "    for i in range(len(eng)) :\n",
    "        cleaned_corpus.append(kor[i] +'\\t'+ eng[i]) # eng + \\t + kor\n",
    "\n",
    "    #세트로 중복 제거\n",
    "    cleaned_corpus = set(cleaned_corpus)\n",
    "\n",
    "    return cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "711f65b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = clean_corpus(kor_path, eng_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30368554",
   "metadata": {},
   "source": [
    "### 정제함수 정의\n",
    "\n",
    "2. 정제 함수를 아래 조건을 만족하게 정의하세요.\n",
    "\n",
    "> * 모든 입력을 소문자로 변환합니다.  \n",
    "> * 알파벳, 문장부호, 한글만 남기고 모두 제거합니다.  \n",
    "> * 문장부호 양옆에 공백을 추가합니다.  \n",
    "> * 문장 앞뒤의 불필요한 공백을 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "6fc80f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "def preprocess_sentence(sentence, kor=False):\n",
    "    \n",
    "    # [[YOUR CODE]]\n",
    "    SOS = '[START]'\n",
    "    EOS = '[END]'\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)    \n",
    "    if kor:    \n",
    "        sentence = re.sub(r\"[^ㄱ-ㅎ가-힣?.!,]+\", \" \", sentence)\n",
    "    else:\n",
    "        sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "      \n",
    "    return SOS + sentence.strip() + EOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121cb87f",
   "metadata": {},
   "source": [
    "### 토큰화 : Sentencepiece\n",
    "\n",
    "3. 한글 말뭉치 kor_corpus 와 영문 말뭉치 eng_corpus 를 각각 분리한 후, 정제하여 토큰화를 진행합니다! 토큰화에는 Sentencepiece를 활용하세요. 첨부된 공식 사이트를 참고해 아래 조건을 만족하는 generate_tokenizer() 함수를 정의합니다. \n",
    "최종적으로 ko_tokenizer 과 en_tokenizer 를 얻으세요. en_tokenizer에는 set_encode_extra_options(\"bos:eos\") 함수를 실행해 타겟 입력이 문장의 시작 토큰과 끝 토큰을 포함할 수 있게 합니다.\n",
    " \n",
    "* google/sentencepiece\n",
    "\n",
    "> * 단어 사전을 매개변수로 받아 원하는 크기의 사전을 정의할 수 있게 합니다. (기본: 20,000)  \n",
    "> * 학습 후 저장된 model 파일을 SentencePieceProcessor() 클래스에 Load()한 후 반환합니다.  \n",
    "> * 특수 토큰의 인덱스를 아래와 동일하게 지정합니다.  \n",
    ">    < PAD > : 0  \n",
    ">    < BOS > : 1  \n",
    ">    < EOS > : 2   \n",
    ">    < UNK > : 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "47c3da91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentencepiece를 활용하여 학습한 tokenizer를 생성합니다.\n",
    "import sentencepiece as spm\n",
    "\n",
    "def generate_tokenizer(corpus,\n",
    "                        vocab_size,\n",
    "                        lang=\"ko\",\n",
    "                        pad_id=0,\n",
    "                        bos_id=1,\n",
    "                        eos_id=2,\n",
    "                        unk_id=3):\n",
    "    # [[YOUR CODE]]\n",
    "    temp_file = os.getenv('HOME')+f'/aiffel/nlp/transformer/data/{lang}_spm.train.ko.temp'\n",
    "\n",
    "    with open(temp_file, 'w') as f:\n",
    "        for row in corpus: \n",
    "            f.write(str(row) + '\\n')\n",
    "\n",
    "    spm.SentencePieceTrainer.Train( \n",
    "        f'--input={temp_file}' + \n",
    "        f' --model_prefix={lang}_spm' +\n",
    "        f' --vocab_size={vocab_size + 6}' + \n",
    "        f' --model_type=unigram' +\n",
    "        # f' --max_sentence_length=50' +            # 문장 최대 길이\n",
    "        f' --pad_id={pad_id} --pad_piece=[PAD]' + # pad\n",
    "        f' --unk_id={unk_id} --unk_piece=[UNK]' + # unknown\n",
    "        f' --bos_id={bos_id} --bos_piece=[BOS]' + # begin of sequence\n",
    "        f' --eos_id={eos_id} --eos_piece=[EOS]' + # end of sequence\n",
    "        f' --user_defined_symbols=[START],[END]') # 사용자 정의 토큰    \n",
    "    \n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load(f'{lang}_spm.model')\n",
    "\n",
    "    return tokenizer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "bac191c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = 29657 - 6\n",
    "\n",
    "eng_corpus = []\n",
    "kor_corpus = []\n",
    "\n",
    "for pair in cleaned_corpus:\n",
    "    k, e = pair.split('\\t')\n",
    "\n",
    "    kor_corpus.append(preprocess_sentence(k, kor=True))\n",
    "    eng_corpus.append(preprocess_sentence(e, kor=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "dfc940b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/Users/ksh/aiffel/nlp/transformer/data/ko_spm.train.ko.temp --model_prefix=ko_spm --vocab_size=29657 --model_type=unigram --pad_id=0 --pad_piece=[PAD] --unk_id=3 --unk_piece=[UNK] --bos_id=1 --bos_piece=[BOS] --eos_id=2 --eos_piece=[EOS] --user_defined_symbols=[START],[END]\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /Users/ksh/aiffel/nlp/transformer/data/ko_spm.train.ko.temp\n",
      "  input_format: \n",
      "  model_prefix: ko_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 29657\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: [START]\n",
      "  user_defined_symbols: [END]\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: [UNK]\n",
      "  bos_piece: [BOS]\n",
      "  eos_piece: [EOS]\n",
      "  pad_piece: [PAD]\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: /Users/ksh/aiffel/nlp/transformer/data/ko_spm.train.ko.temp\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 78968 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: [PAD]\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: [BOS]\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: [EOS]\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: [UNK]\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: [START]\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: [END]\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=4980852\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.9502% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=1162\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.999502\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 78968 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 140519 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 78968\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 201015\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 201015 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=79844 obj=15.75 num_tokens=427034 num_tokens/piece=5.34835\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=71544 obj=14.9338 num_tokens=430022 num_tokens/piece=6.01059\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=53656 obj=14.9725 num_tokens=449839 num_tokens/piece=8.38376\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=53622 obj=14.9326 num_tokens=450935 num_tokens/piece=8.40951\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=40216 obj=14.9677 num_tokens=476589 num_tokens/piece=11.8507\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=40216 obj=14.9123 num_tokens=476979 num_tokens/piece=11.8604\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=32622 obj=15.044 num_tokens=495587 num_tokens/piece=15.1918\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=32622 obj=15.0151 num_tokens=496399 num_tokens/piece=15.2167\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: ko_spm.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: ko_spm.vocab\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/Users/ksh/aiffel/nlp/transformer/data/en_spm.train.ko.temp --model_prefix=en_spm --vocab_size=29657 --model_type=unigram --pad_id=0 --pad_piece=[PAD] --unk_id=3 --unk_piece=[UNK] --bos_id=1 --bos_piece=[BOS] --eos_id=2 --eos_piece=[EOS] --user_defined_symbols=[START],[END]\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /Users/ksh/aiffel/nlp/transformer/data/en_spm.train.ko.temp\n",
      "  input_format: \n",
      "  model_prefix: en_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 29657\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: [START]\n",
      "  user_defined_symbols: [END]\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: [UNK]\n",
      "  bos_piece: [BOS]\n",
      "  eos_piece: [EOS]\n",
      "  pad_piece: [PAD]\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: /Users/ksh/aiffel/nlp/transformer/data/en_spm.train.ko.temp\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 78968 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: [PAD]\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: [BOS]\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: [EOS]\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: [UNK]\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: [START]\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: [END]\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=10819433\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.9668% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=52\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.999668\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 78968 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 90096 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 78968\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 59459\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 59459 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=42624 obj=12.5856 num_tokens=128622 num_tokens/piece=3.0176\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=34197 obj=10.7432 num_tokens=129829 num_tokens/piece=3.7965\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=29915 obj=10.67 num_tokens=131043 num_tokens/piece=4.38051\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=29651 obj=10.6524 num_tokens=131658 num_tokens/piece=4.44025\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: en_spm.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: en_spm.vocab\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_tokenizer = generate_tokenizer(kor_corpus, SRC_VOCAB_SIZE, 'ko')\n",
    "en_tokenizer = generate_tokenizer(eng_corpus, TGT_VOCAB_SIZE, 'en')\n",
    "en_tokenizer.set_encode_extra_options('bos:eos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6e227e",
   "metadata": {},
   "source": [
    "### 코퍼스 구측 : 토큰 길이 50 이하\n",
    "\n",
    "4. 토크나이저를 활용해 토큰의 길이가 50 이하인 데이터를 선별하여 src_corpus 와 tgt_corpus 를 각각 구축하고, 텐서 enc_train 과 dec_train 으로 변환하세요! (❗모든 데이터를 사용할 경우 학습에 굉장히 오랜 시간이 걸립니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "bdac53c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d076a7cba814ef78344a9453863b809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78968 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm    # Process 과정을 보기 위해\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "assert len(kor_corpus) == len(eng_corpus)\n",
    "\n",
    "# 토큰의 길이가 50 이하인 문장만 남깁니다. \n",
    "for idx in tqdm(range(len(kor_corpus))):\n",
    "    # [[YOUR CODE]]\n",
    "    source = ko_tokenizer.encode(kor_corpus[idx])\n",
    "    target = en_tokenizer.encode(eng_corpus[idx])\n",
    "    \n",
    "    if len(source) < 51:\n",
    "        src_corpus.append(source)\n",
    "        tgt_corpus.append(target)\n",
    "\n",
    "# 패딩처리를 완료하여 학습용 데이터를 완성합니다. \n",
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "cfa43ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START]여론조사에 의하면 미국 유권자의 분의 은 오바마가 미국에 대한 애국심이 부족하다고 밝혔다 .[END]\n",
      "[START]One quarter of all registered voters say Obama lacks patriotism , according to the poll .[END]\n"
     ]
    }
   ],
   "source": [
    "print(ko_tokenizer.decode(src_corpus[0]))\n",
    "print(en_tokenizer.decode(tgt_corpus[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5955e1",
   "metadata": {},
   "source": [
    "## Step 3. 모델 설계\n",
    "---\n",
    "오늘 배운 내용을 활용해서 Transformer 모델을 설계해보세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5270500e",
   "metadata": {},
   "source": [
    "### 트랜스포머 내부 모듈 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadec617",
   "metadata": {},
   "source": [
    "#### 위치 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "851ea517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#position encoding\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc420e7",
   "metadata": {},
   "source": [
    "#### 멀티헤드 어텐션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "c8c73adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "            \n",
    "        self.depth = d_model // self.num_heads\n",
    "            \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "            \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "            \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (batch_size, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "        \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "            \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)  \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "                \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe8281b",
   "metadata": {},
   "source": [
    "#### 위치기반 피드 포워드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "e5128d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bd6a87",
   "metadata": {},
   "source": [
    "### 트랜스포머 레이어 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75027260",
   "metadata": {},
   "source": [
    "#### 인코더 레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "7c378d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d78c7c",
   "metadata": {},
   "source": [
    "#### 디코더 레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "437cd84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads) #+\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6) # +\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask) # masked \n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, causality_mask) # Q,V encoder, mask\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd44542",
   "metadata": {},
   "source": [
    "### 트랜스포머 클래스 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baefdd01",
   "metadata": {},
   "source": [
    "#### 인코더 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "823b724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b32a6e1",
   "metadata": {},
   "source": [
    "#### 디코더 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "39507f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be47a1d7",
   "metadata": {},
   "source": [
    "### 트랜스포머 모델 구현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "0c2e2c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,         # 6\n",
    "                    d_model,          # 512\n",
    "                    n_heads,          # 8 \n",
    "                    d_ff,             # 2048\n",
    "                    src_vocab_size,   # 30000\n",
    "                    tgt_vocab_size,   # 30000\n",
    "                    pos_len,          # \n",
    "                    dropout=0.1,\n",
    "                    shared=True):        \n",
    "        \n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        \"\"\"\n",
    "        1. Embedding Layer 정의\n",
    "        2. Positional Encoding 정의\n",
    "        3. Encoder / Decoder 정의\n",
    "        4. Output Linear 정의\n",
    "        5. Shared Weights\n",
    "        6. Dropout 정의\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Embedding Layer 정의\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model) # (30000, 512)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model) # (30000, 512)\n",
    "\n",
    "        # 2. Positional Encoding 정의\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)         # (1024, 512)\n",
    "               \n",
    "        # 3. Encoder / Decoder 정의\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout) # (6, 512, 8, 0.1)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout) # (6, 512, 8, 0.1)\n",
    "        \n",
    "        # 4. Output Linear 정의\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)                   # (30000)\n",
    "\n",
    "        # 5. Shared Weights\n",
    "        self.shared = shared\n",
    "        if shared: \n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "        \n",
    "        #6. Dropout 정의\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)                   # 0.1\n",
    "        \n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940611fd",
   "metadata": {},
   "source": [
    "### 보조 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7223a84",
   "metadata": {},
   "source": [
    "#### 마스크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "45c3042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32) # returns true if value is equal to 0; tf cast is just another way to transform tensors into designated dtype\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask) # returns max value between two\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1]) # why two? because of two multihead?\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62656e03",
   "metadata": {},
   "source": [
    "#### 학습 스케쥴러"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "d4e1439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5df767a",
   "metadata": {},
   "source": [
    "## Step 4. 훈련하기\n",
    "---\n",
    "\n",
    "앞서 필요한 것들을 모두 정의했기 때문에 우리는 훈련만 하면 됩니다! 아래 과정을 차근차근 따라가며 모델을 훈련하고, 예문에 대한 멋진 번역을 제출하세요!\n",
    "\n",
    "### 1) 2 Layer를 가지는 Transformer를 선언하세요.\n",
    "(하이퍼파라미터는 자유롭게 조절합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "4083d82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[YOUR CODE]]\n",
    "\n",
    "n_layers = 2\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "d_ff = 2048\n",
    "src_vocab_size = ko_tokenizer.get_piece_size()\n",
    "tgt_vocab_size = en_tokenizer.get_piece_size()\n",
    "pos_len = 512\n",
    "dropout=0.1\n",
    "\n",
    "transformer = Transformer(n_layers, d_model, n_heads, d_ff, src_vocab_size, tgt_vocab_size, pos_len, dropout, shared=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbc24ae",
   "metadata": {},
   "source": [
    "### 2) 논문에서 사용한 것과 동일한 Learning Rate Scheduler를 선언하고, 이를 포함하는 Adam Optimizer를 선언하세요.\n",
    " (Optimizer의 파라미터 역시 논문과 동일하게 설정합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "8cdaefd4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "steps = ko_tokenizer.get_piece_size() // BATCH_SIZE\n",
    "\n",
    "learning_rate = LearningRateScheduler(steps)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7382d7",
   "metadata": {},
   "source": [
    "### 3) Loss 함수를 정의하세요.\n",
    "\n",
    "Sequence-to-sequence 모델에서 사용했던 Loss와 유사하되, Masking 되지 않은 입력의 개수로 Scaling하는 과정을 추가합니다. (트랜스포머가 모든 입력에 대한 Loss를 한 번에 구하기 때문입니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "4e68b567",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # Masking 되지 않은 입력의 개수로 Scaling하는 과정\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5ad1b6",
   "metadata": {},
   "source": [
    "### 4) Train_step 함수를 정의\n",
    "\n",
    "train_step 함수를 정의하세요.\n",
    "입력 데이터에 알맞은 Mask를 생성하고, 이를 모델에 전달하여 연산에서 사용할 수 있게 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "776fea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Step 함수 정의\n",
    "\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    # 계산된 loss에 tf.GradientTape()를 적용해 학습을 진행합니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    # 최종적으로 optimizer.apply_gradients()가 사용됩니다. \n",
    "    # [[YOUR CODE]]\n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "    \n",
    "    variables = model.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c048d30",
   "metadata": {},
   "source": [
    "### 5) 학습 진행\n",
    "\n",
    "학습을 진행합니다.\n",
    "매 Epoch 마다 제시된 예문에 대한 번역을 생성하고, 멋진 번역이 생성되면 그때의 하이퍼파라미터와 생성된 번역을 제출하세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f061a3d",
   "metadata": {},
   "source": [
    "예문\n",
    "\n",
    "1. 오바마는 대통령이다.\n",
    "2. 시민들은 도시 속에 산다.\n",
    "3. 커피는 필요 없다.\n",
    "4. 일곱 명의 사망자가 발생했다.\n",
    "결과(output)\n",
    "\n",
    "Translations\n",
    "> 1. obama is the president elect .\n",
    "> 2. they are in the city .\n",
    "> 3. they don t need to be a lot of drink .\n",
    "> 4. seven other people have been killed in the attacks .\n",
    "\n",
    "Hyperparameters\n",
    "> n_layers: 2  \n",
    "> d_model: 512  \n",
    "> n_heads: 8  \n",
    "> d_ff: 2048  \n",
    "> dropout: 0.3  \n",
    "\n",
    "Training Parameters\n",
    "> Warmup Steps: 4000  \n",
    "> Batch Size: 64  \n",
    "> Epoch At: 5  \n",
    "번역 생성에는 아래 소스를 사용하시길 바랍니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a301ad6",
   "metadata": {},
   "source": [
    "#### 학습 과정 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dffc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/h8hfx8zn0pb8vvgjpyjwq7cc0000gn/T/ipykernel_1920/1975109176.py:18: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  t = tqdm_notebook(idx_list)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52858555d8dc4d3ca6dbbd49a4a25de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-30 21:29:48.170724: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-01-30 21:29:48.174488: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "from tqdm import tqdm_notebook \n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "examples = [\n",
    "            \"오바마는 대통령이다.\",\n",
    "            \"시민들은 도시 속에 산다.\",\n",
    "            \"커피는 필요 없다.\",\n",
    "            \"일곱 명의 사망자가 발생했다.\"\n",
    "]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE)) # 64\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "\n",
    "    for example in examples:\n",
    "        translate(example, transformer, ko_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8000cc",
   "metadata": {},
   "source": [
    "## Step 5. 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5a6c12",
   "metadata": {},
   "source": [
    "### 시각화\n",
    "번역 생성에는 아래 소스를 사용하시길 바랍니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59baaeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 시각화 함수\n",
    "\n",
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data, \n",
    "                        square=True,\n",
    "                        vmin=0.0, vmax=1.0, \n",
    "                        cbar=False, ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143b3430",
   "metadata": {},
   "source": [
    "### 번역 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "171339d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 함수\n",
    "\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4f67f1",
   "metadata": {},
   "source": [
    "### 결과 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61f8cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 및 Attention 시각화 결합\n",
    "\n",
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b1537",
   "metadata": {},
   "source": [
    "translate() 함수의 plot_attention 변수를 True 로 주면 번역 결과에 대한 Attention Map을 시각화 해볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4604fa6d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41011dc4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bd91f53",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bff5e841",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "267.390625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
