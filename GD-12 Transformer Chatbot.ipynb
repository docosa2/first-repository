{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7a395c6",
   "metadata": {},
   "source": [
    "# GD-12 Transformer Chatbot\n",
    "\n",
    "### 2023-02-09 (목)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b971b47",
   "metadata": {},
   "source": [
    "## 라이브러리 버전을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70102c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip uninstall gensim \n",
    "# ! pip install --upgrade gensim==3.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c91b1f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "import nltk\n",
    "import gensim\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0559ed04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.22.4\n",
      "1.5.1\n",
      "2.9.0\n",
      "3.7\n",
      "3.8.3\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(tf.__version__)\n",
    "print(nltk.__version__)\n",
    "print(gensim.__version__) # pip install --upgrade gensim==3.8.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77df4eb8",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c26c4e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /data/ChatbotData.csv\n",
    "# !wget https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0db2bc6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df_chatbotdata = pd.read_csv('./data/ChatbotData.csv')\n",
    "df_chatbotdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd58e833",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 정제\n",
    "---\n",
    "아래 조건을 만족하는 preprocess_sentence() 함수를 구현하세요.\n",
    "\n",
    "영문자의 경우, 모두 소문자로 변환합니다.  \n",
    "영문자와 한글, 숫자, 그리고 주요 특수문자를 제외하곤 정규식을 활용하여 모두 제거합니다. \n",
    "문장부호 양옆에 공백을 추가하는 등 이전과 다르게 생략된 기능들은 우리가 사용할 토크나이저가 지원하기 때문에 굳이 구현하지 않아도 괜찮습니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2bb7397d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def to_lower(sentence: str) -> str:\n",
    "    return sentence.lower()\n",
    "\n",
    "def add_white_space(sentence: str) -> str:\n",
    "    return re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "def to_korean_alpha_numeric(sentence: str) -> str:\n",
    "    return re.sub(r\"[^ㄱ-ㅎ가-힣a-zA-Z!.,?0-9]+\", \" \", sentence)\n",
    "\n",
    "def strip(sentence: str) -> str:\n",
    "    return sentence.strip()\n",
    "\n",
    "def preprocess_sentence(sentence: str) -> str:\n",
    "    sentence = to_lower(sentence)\n",
    "    sentence = add_white_space(sentence)\n",
    "    sentence = to_korean_alpha_numeric(sentence)\n",
    "    sentence = strip(sentence)\n",
    "    return sentence\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41af4d6",
   "metadata": {},
   "source": [
    "## Step 3. 데이터 토큰화\n",
    "---\n",
    "토큰화에는 KoNLPy의 mecab 클래스를 사용합니다.\n",
    "\n",
    "아래 조건을 만족하는 build_corpus() 함수를 구현하세요!  \n",
    "\n",
    "1. 소스 문장 데이터와 타겟 문장 데이터를 입력으로 받습니다.  \n",
    "2. 데이터를 앞서 정의한 preprocess_sentence() 함수로 정제하고, 토큰화합니다.  \n",
    "3. 토큰화는 전달받은 토크나이즈 함수를 사용합니다. 이번엔 mecab.morphs 함수를 전달하시면 됩니다.  \n",
    "4. 토큰의 개수가 일정 길이 이상인 문장은 데이터에서 제외합니다.  \n",
    "5. 중복되는 문장은 데이터에서 제외합니다. 소스 : 타겟 쌍을 비교하지 않고 소스는 소스대로 타겟은 타겟대로 검사합니다. 중복 쌍이 흐트러지지 않도록 유의하세요!  \n",
    "\n",
    "구현한 함수를 활용하여 questions 와 answers 를 각각 que_corpus , ans_corpus 에 토큰화하여 저장합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "30a90238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "92e16b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = df_chatbotdata['Q'].values\n",
    "answers = df_chatbotdata['A'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1c4d4d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(src, tgt, tokenizer, max_len= 50) :\n",
    "    \n",
    "    temp_corpus = list(set(src + '\\t' + tgt)) #중복 제거\n",
    "    \n",
    "    src_corpus, tgt_corpus = list(), list()\n",
    "    \n",
    "    for sen in temp_corpus :\n",
    "        src_sen, tgt_sen = sen.split('\\t')\n",
    "        \n",
    "        #src\n",
    "        src_sen = preprocess_sentence(src_sen) # 전처리\n",
    "        src_sen = tokenizer.morphs(src_sen) # 형태소 토크나이\n",
    "\n",
    "        tgt_sen = preprocess_sentence(tgt_sen)\n",
    "        tgt_sen = tokenizer.morphs(tgt_sen)\n",
    "        \n",
    "        if len(src_sen) < max_len and len(tgt_sen) < max_len :\n",
    "            src_corpus.append(' '.join(src_sen))\n",
    "            tgt_corpus.append(' '.join(tgt_sen))\n",
    "        \n",
    "    assert len(src_corpus) == len(tgt_corpus)   \n",
    "    \n",
    "    return src_corpus, tgt_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7cb708c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call build_corpus\n",
    "corpus_questions, corpus_answers = build_corpus(questions, answers, mecab, max_len = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "806ed503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11750\n"
     ]
    }
   ],
   "source": [
    "assert len(corpus_questions) == len(corpus_answers)\n",
    "\n",
    "print(len(corpus_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0b9998c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 농구 하 다 무르팍 깨 짐\n",
      "A: 약 바르 고 얼른 나 으세요 .\n",
      " \n",
      "Q: 그녀 생각 만 하 면 멍 해져\n",
      "A: 큐피드 의 화살 에 맞 았 나 봐요 .\n",
      " \n",
      "Q: 도대체 왜 이러 는 걸까\n",
      "A: 무슨 일 이 있 었 나 봐요 .\n",
      " \n",
      "Q: 결혼 은 타이밍 인가 ?\n",
      "A: 결혼 뿐 만 아니 라 인생 은 타이밍 의 연속 이 예요 .\n",
      " \n",
      "Q: 그녀 를 다시 찾 았 습니다\n",
      "A: 다행 이 네요 .\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('Q:', corpus_questions[i])\n",
    "    print('A:', corpus_answers[i])\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eaa593",
   "metadata": {},
   "source": [
    "## Step 4. Augmentation\n",
    "---\n",
    "우리에게 주어진 데이터는 1만 개가량으로 적은 편에 속합니다. 이럴 때에 사용할 수 있는 테크닉을 배웠으니 활용해 봐야겠죠? Lexical Substitution을 실제로 적용해 보도록 하겠습니다.\n",
    "\n",
    "아래 링크를 참고하여 한국어로 사전 훈련된 Embedding 모델을 다운로드합니다. Korean (w) 가 Word2Vec으로 학습한 모델이며 용량도 적당하므로 사이트에서 Korean (w)를 찾아 다운로드하고, ko.bin 파일을 얻으세요!\n",
    "\n",
    "* [Kyubyong/wordvectors](https://github.com/Kyubyong/wordvectors)\n",
    "\n",
    "다운로드한 모델을 활용해 데이터를 Augmentation 하세요! 앞서 정의한 lexical_sub() 함수를 참고하면 도움이 많이 될 겁니다.\n",
    "\n",
    "Augmentation된 que_corpus 와 원본 ans_corpus 가 병렬을 이루도록, 이후엔 반대로 원본 que_corpus 와 Augmentation된 ans_corpus 가 병렬을 이루도록 하여 전체 데이터가 원래의 3배가량으로 늘어나도록 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e9935acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "model_path = os.getenv('HOME') + \"/aiffel/transformer_chatbot/model/ko.bin\"\n",
    "model_path = './model/ko.bin'\n",
    "wv = Word2Vec.load(model_path) # gensim 3.8.3 이하 버전에서만 동작함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8f12f293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/h8hfx8zn0pb8vvgjpyjwq7cc0000gn/T/ipykernel_3275/1631014209.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  wv.most_similar('영화') # similar words - korean\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('다큐멘터리', 0.7265259027481079),\n",
       " ('영화사', 0.7152142524719238),\n",
       " ('드라마', 0.705294132232666),\n",
       " ('뮤지컬', 0.6947016716003418),\n",
       " ('코미디', 0.6909325122833252),\n",
       " ('영화인', 0.6702202558517456),\n",
       " ('서부극', 0.6571459174156189),\n",
       " ('스릴러', 0.6533164978027344),\n",
       " ('로맨스', 0.6428800225257874),\n",
       " ('애니메이션', 0.6425570249557495)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('영화') # similar words - korean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80014505",
   "metadata": {},
   "source": [
    "### Lexical Substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b0561a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_sub(sentence, word2vec):\n",
    "    res = \"\"\n",
    "    toks = mecab.morphs(sentence)\n",
    "\n",
    "    try:\n",
    "        _from = random.choice(toks)\n",
    "        _to = word2vec.most_similar(_from)[0][0]\n",
    "        \n",
    "    except:   # 단어장에 단어가 없으면 None을 리턴 \n",
    "        return None\n",
    "\n",
    "    for tok in toks:\n",
    "        if tok is _from: res += _to + \" \"\n",
    "        else: res += tok + \" \"\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aaa5e817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "친구 와 함께 영화 을 보 았 다 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/h8hfx8zn0pb8vvgjpyjwq7cc0000gn/T/ipykernel_3275/1162967677.py:7: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  _to = word2vec.most_similar(_from)[0][0]\n"
     ]
    }
   ],
   "source": [
    "sentence = '친구와 함께 영화를 보았다'\n",
    "res = lexical_sub(sentence, wv)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "453802a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca4c6b27204042beb9f35d5a0dd3686a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/h8hfx8zn0pb8vvgjpyjwq7cc0000gn/T/ipykernel_3275/1162967677.py:7: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  _to = word2vec.most_similar(_from)[0][0]\n"
     ]
    }
   ],
   "source": [
    "new_corpus = []\n",
    "\n",
    "for old_question, old_answer in tqdm(zip(corpus_questions, corpus_answers)):\n",
    "    new_question = lexical_sub(old_question, wv)\n",
    "    new_answer = lexical_sub(old_answer, wv)\n",
    "    if new_question is not None: \n",
    "        new_corpus.append(new_question +'\\t'+ old_answer)\n",
    "    if new_answer is not None: \n",
    "        new_corpus.append(old_question +'\\t'+ new_answer)\n",
    "    # Augmentation이 없더라도 원본 문장을 포함시킵니다\n",
    "    new_corpus.append(old_question +'\\t'+ old_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "17bed012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32122\n",
      "32119\n"
     ]
    }
   ],
   "source": [
    "print(len(new_corpus))\n",
    "new_corpus = list(set(new_corpus))\n",
    "print(len(new_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cad99c4",
   "metadata": {},
   "source": [
    "## Step 5. 데이터 벡터화\n",
    "---\n",
    "타겟 데이터인 ans_corpus 에 <start> 토큰과 <end> 토큰이 추가되지 않은 상태이니 이를 먼저 해결한 후 벡터화를 진행합니다. 우리가 구축한 ans_corpus 는 list 형태이기 때문에 아주 쉽게 이를 해결할 수 있답니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7c0912c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start>', '12', '시', '땡', '!', '<end>']\n"
     ]
    }
   ],
   "source": [
    "sample_data = [\"12\", \"시\", \"땡\", \"!\"]\n",
    "\n",
    "print([\"<start>\"] + sample_data + [\"<end>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a772db",
   "metadata": {},
   "source": [
    "위 소스를 참고하여 타겟 데이터 전체에 <start> 토큰과 <end> 토큰을 추가해 주세요!\n",
    "챗봇 훈련 데이터의 가장 큰 특징 중 하나라고 하자면 바로 소스 데이터와 타겟 데이터가 같은 언어를 사용한다는 것이겠죠. 앞서 배운 것처럼 이는 Embedding 층을 공유했을 때 많은 이점을 얻을 수 있습니다.\n",
    "\n",
    "특수 토큰을 더함으로써 ans_corpus 또한 완성이 되었으니, que_corpus 와 결합하여 전체 데이터에 대한 단어 사전을 구축하고 벡터화하여 enc_train 과 dec_train 을 얻으세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8bae8942",
   "metadata": {},
   "outputs": [],
   "source": [
    "que_corpus = []\n",
    "ans_corpus = []\n",
    "\n",
    "for QnA in new_corpus : \n",
    "    question, answer = QnA.split('\\t') #separate again into question and answer corpus\n",
    "    que_corpus.append(question)\n",
    "    ans_corpus.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2603c149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> 사랑 이 깊 네요 . <end>\n"
     ]
    }
   ],
   "source": [
    "ans_corpus = ['<start> ' + answer + ' <end>' for answer in ans_corpus]\n",
    "print(ans_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c358a4",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b95e214b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3211\n"
     ]
    }
   ],
   "source": [
    "test_size = int(len(que_corpus) * 0.1)\n",
    "print(test_size)\n",
    "\n",
    "que_train = que_corpus[:-test_size]\n",
    "ans_train = ans_corpus[:-test_size]\n",
    "\n",
    "que_test = que_corpus[-test_size:]\n",
    "ans_test = ans_corpus[-test_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c42856",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "47a3b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate tokenizer\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts(que_corpus + ans_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2ea4325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat seq tokens\n",
    "enc_tensor = tokenizer.texts_to_sequences(que_train)\n",
    "dec_tensor = tokenizer.texts_to_sequences(ans_train)\n",
    "\n",
    "enc_tensor =  tf.keras.preprocessing.sequence.pad_sequences(enc_tensor, maxlen=52, padding='post')\n",
    "dec_tensor =  tf.keras.preprocessing.sequence.pad_sequences(dec_tensor, maxlen=52, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cafe285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((enc_tensor, dec_tensor)).batch(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6df299",
   "metadata": {},
   "source": [
    "## Step 6. 훈련하기\n",
    "---\n",
    "앞서 번역 모델을 훈련하며 정의한 Transformer 를 그대로 사용하시면 됩니다! 대신 데이터의 크기가 작으니 하이퍼파라미터를 튜닝해야 과적합을 피할 수 있습니다. 모델을 훈련하고 아래 예문에 대한 답변을 생성하세요! 가장 멋진 답변과 모델의 하이퍼파라미터를 제출하시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c4c21d",
   "metadata": {},
   "source": [
    "### 모델 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1443a8d",
   "metadata": {},
   "source": [
    "#### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c667005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d2d082",
   "metadata": {},
   "source": [
    "#### Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "92344e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_lookahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])\n",
    "    dec_tgt_padding_mask = generate_padding_mask(tgt)\n",
    "    dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a063eb9",
   "metadata": {},
   "source": [
    "#### MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "33430d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "                        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "            \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccec47c",
   "metadata": {},
   "source": [
    "#### Position-wise Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "15904472",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7377bb3c",
   "metadata": {},
   "source": [
    "#### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "955d30af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17eb417",
   "metadata": {},
   "source": [
    "#### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4053dfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        '''\n",
    "        Masked Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        # Q, K, V 순서에 주의하세요!\n",
    "        out, dec_enc_attn = self.enc_dec_attn(Q=out, K=enc_out, V=enc_out, mask=dec_enc_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7429242f",
   "metadata": {},
   "source": [
    "#### Encoder 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c79c59e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b66db8",
   "metadata": {},
   "source": [
    "#### Decoder 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "07e45891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, dec_enc_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c20ec2",
   "metadata": {},
   "source": [
    "#### 트랜스포머 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4452593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, dec_enc_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060d476f",
   "metadata": {},
   "source": [
    "### 하이퍼 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0e87a24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7963\n"
     ]
    }
   ],
   "source": [
    "# VOCAB_SIZE \n",
    "VOCAB_SIZE = len(tokenizer.index_word)\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "301c8aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "\n",
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2f5e5a",
   "metadata": {},
   "source": [
    "#### LR Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "26534987",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = LearningRateScheduler(d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2788ed9c",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "14f3a668",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c92d9c",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7b9895e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6916cde1",
   "metadata": {},
   "source": [
    "### Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cbbd5e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501e388f",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ec5b9fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = tf.train.Checkpoint(step = tf.Variable(1), optimizer = optimizer , transformer = transformer)\n",
    "manager = tf.train.CheckpointManager(ckpt, './checkpoints',max_to_keep=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcaad47",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5979270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_checkpoint(transformer, manager, EPOCH = 2):\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    if manager.latest_checkpoint:\n",
    "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    \n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")\n",
    "\n",
    "    \n",
    "    for epoch in range(EPOCH):\n",
    "        ckpt.step.assign_add(1)\n",
    "        total_loss = 0\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        dataset_count = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "        tqdm_bar = tqdm(total=dataset_count)\n",
    "        for step, (enc_batch, dec_batch) in enumerate(train_dataset):\n",
    "            batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "            train_step(enc_batch,\n",
    "                        dec_batch,\n",
    "                        transformer,\n",
    "                        optimizer)\n",
    "\n",
    "            total_loss += batch_loss\n",
    "\n",
    "            tqdm_bar.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "            tqdm_bar.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (step + 1)))\n",
    "            tqdm_bar.update()\n",
    "            \n",
    "        if int(ckpt.step) % 2 == 0:\n",
    "            save_path = manager.save()\n",
    "            print(\"Saved checkpoint for step {}: {}\".format(int(ckpt.step), save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e96b69e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing from scratch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24e812e3fb643fdba171470cc0721c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-10 08:48:21.905834: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-02-10 08:48:21.908957: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-10 08:50:35.600359: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint for step 2: ./checkpoints/ckpt-1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b51da76ac64b11abbd061c4583f78d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab6d4c1707e41bf821433d453de2f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint for step 4: ./checkpoints/ckpt-2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ac0c1269514d17971861919aa9d488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51928534520443d19fd22a631343b2d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint for step 6: ./checkpoints/ckpt-3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c0150f6b91491db034758de51ba2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6670015c05a842be9a0df74c91ee396b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint for step 8: ./checkpoints/ckpt-4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089b610204554c53896a0a3c051cb352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccfb81f0d051459392e53cbc1e613f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint for step 10: ./checkpoints/ckpt-5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8bac01c86bf4ef2bd96d1915faa778c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a81ede506e340acb1560273cbc339c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint for step 12: ./checkpoints/ckpt-6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14eefc59800947759ad7088592494b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a80012c7503458aa63b298e38321f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint for step 14: ./checkpoints/ckpt-7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e944e4247ea44ef97a88b89eee0076a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a9220cce64441387777dfbaacb506c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint for step 16: ./checkpoints/ckpt-8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab8974c70ed4ff2940c25931eb840e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39362c8e40724f95b560804af3ffad03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint for step 18: ./checkpoints/ckpt-9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646bd758e5b6406693e4d6081984b4e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc5ed6798a046a2b73d7ec8b36dd175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint for step 20: ./checkpoints/ckpt-10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8a848adfb9413e9f7048f35d99aca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_checkpoint(transformer, manager, EPOCH = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afad30f9",
   "metadata": {},
   "source": [
    "###### 예문\n",
    "1. 지루하다, 놀러가고 싶어.\n",
    "2. 오늘 일찍 일어났더니 피곤하다.\n",
    "3. 간만에 여자친구랑 데이트 하기로 했어.\n",
    "4. 집에 있는다는 소리야.\n",
    "\n",
    "---\n",
    "\n",
    "###### 제출\n",
    "\n",
    "Translations\n",
    "> 1. 잠깐 쉬 어도 돼요 . <end>\n",
    "> 2. 맛난 거 드세요 . <end>\n",
    "> 3. 떨리 겠 죠 . <end>\n",
    "> 4. 좋 아 하 면 그럴 수 있 어요 . <end>\n",
    "\n",
    "Hyperparameters\n",
    "> n_layers: 1\n",
    "> d_model: 368\n",
    "> n_heads: 8\n",
    "> d_ff: 1024\n",
    "> dropout: 0.2\n",
    "\n",
    "Training Parameters\n",
    "> Warmup Steps: 1000\n",
    "> Batch Size: 64\n",
    "> Epoch At: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff42122",
   "metadata": {},
   "source": [
    "## Step 7. 성능 측정하기\n",
    "---\n",
    "챗봇의 경우, 올바른 대답을 하는지가 중요한 평가 지표입니다. 올바른 답변을 하는지 눈으로 확인할 수 있겠지만, 많은 데이터의 경우는 모든 결과를 확인할 수 없을 것입니다. 주어진 질문에 적절한 답변을 하는지 확인하고, BLEU Score를 계산하는 calculate_bleu() 함수도 적용해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6cf310f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def translate(tokens, model, src_tokenizer, tgt_tokenizer):\n",
    "    tokens = mecab.morphs(tokens)\n",
    "    tokens = tokenizer.texts_to_sequences(tokens)\n",
    "    padded_tokens = tf.keras.preprocessing.sequence.pad_sequences(tokens,\n",
    "                                                           maxlen=50,\n",
    "                                                           padding='post')\n",
    "    ids = []\n",
    "    output = tf.expand_dims([tokenizer.word_index['<start>']], 0)   \n",
    "    for i in range(50):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(padded_tokens, output)\n",
    "\n",
    "        predictions, _, _, _ = model(padded_tokens, \n",
    "                                      output,\n",
    "                                      enc_padding_mask,\n",
    "                                      combined_mask,\n",
    "                                      dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tokenizer.word_index['<end>'] == predicted_id:\n",
    "            result = ' '.join([tgt_tokenizer.index_word[i] for i in ids if i != 0])  \n",
    "            return result\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = ' '.join([tgt_tokenizer.index_word[i] for i in ids if i != 0])  \n",
    "    return result\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "75437e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quesions 1: 지루하다, 놀러가고 싶어.\n",
      "reponse 1: 마음 이 가까운 곳 하 는 마음 을 가 보 세요 .\n",
      "\n",
      "quesions 2: 오늘 일찍 일어났더니 피곤하다.\n",
      "reponse 2: 오늘 도 즐거운 하루 보내 세요 .\n",
      "\n",
      "quesions 3: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "reponse 3: 좋 은 날 이 죠 .\n",
      "\n",
      "quesions 4: 집에 있는다는 소리야.\n",
      "reponse 4: 마련 이 편하 겠 네요 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# examples :\n",
    "\n",
    "examples = [\n",
    "    '지루하다, 놀러가고 싶어.',\n",
    "    '오늘 일찍 일어났더니 피곤하다.',\n",
    "    '간만에 여자친구랑 데이트 하기로 했어.',\n",
    "    '집에 있는다는 소리야.'\n",
    "]\n",
    "\n",
    "for i,example in enumerate(examples):\n",
    "    \n",
    "    print(f'quesions {i+1}:',example)\n",
    "    print(f'reponse {i+1}:', translate(example, transformer, tokenizer, tokenizer))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b98501",
   "metadata": {},
   "source": [
    "### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1bb3386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decoder(sentence, \n",
    "                        src_len,\n",
    "                        tgt_len,\n",
    "                        model,\n",
    "                        src_tokenizer,\n",
    "                        tgt_tokenizer,\n",
    "                        beam_size):\n",
    "    \n",
    "    sentence = mecab.morphs(sentence)\n",
    "    \n",
    "    def calc_prob(src_ids, tgt_ids, model):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(src_ids, tgt_ids)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(src_ids, \n",
    "                tgt_ids,\n",
    "                enc_padding_mask,\n",
    "                combined_mask,\n",
    "                dec_padding_mask)\n",
    "\n",
    "        return tf.math.softmax(predictions, axis=-1)\n",
    "    \n",
    "    tokens = src_tokenizer.texts_to_sequences(sentence)\n",
    "    \n",
    "    src_in = tf.keras.preprocessing.sequence.pad_sequences(tokens,\n",
    "                                                            maxlen=src_len,\n",
    "                                                            padding='post')\n",
    "\n",
    "    pred_cache = np.zeros((beam_size * beam_size, tgt_len), dtype=np.int64)\n",
    "    pred_tmp = np.zeros((beam_size, tgt_len), dtype=np.int64)\n",
    "\n",
    "    eos_flag = np.zeros((beam_size, ), dtype=np.int64)\n",
    "    scores = np.ones((beam_size, ))\n",
    "\n",
    "    pred_tmp[:, 0] = tgt_tokenizer.word_index['<start>']\n",
    "\n",
    "    dec_in = tf.expand_dims(pred_tmp[0, :1], 0)\n",
    "    prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "    for seq_pos in range(1, tgt_len):\n",
    "        score_cache = np.ones((beam_size * beam_size, ))\n",
    "\n",
    "        # init\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx*beam_size\n",
    "\n",
    "            score_cache[cache_pos:cache_pos+beam_size] = scores[branch_idx]\n",
    "            pred_cache[cache_pos:cache_pos+beam_size, :seq_pos] = \\\n",
    "            pred_tmp[branch_idx, :seq_pos]\n",
    "\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx*beam_size\n",
    "\n",
    "            if seq_pos != 1:   # 모든 Branch를 로 시작하는 경우를 방지\n",
    "                dec_in = pred_cache[branch_idx, :seq_pos]\n",
    "                dec_in = tf.expand_dims(dec_in, 0)\n",
    "\n",
    "                prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "            for beam_idx in range(beam_size):\n",
    "                max_idx = np.argmax(prob)\n",
    "\n",
    "                score_cache[cache_pos+beam_idx] *= prob[max_idx]\n",
    "                pred_cache[cache_pos+beam_idx, seq_pos] = max_idx\n",
    "\n",
    "                prob[max_idx] = -1\n",
    "\n",
    "        for beam_idx in range(beam_size):\n",
    "            if eos_flag[beam_idx] == -1: continue\n",
    "\n",
    "            max_idx = np.argmax(score_cache)\n",
    "            prediction = pred_cache[max_idx, :seq_pos+1]\n",
    "\n",
    "            pred_tmp[beam_idx, :seq_pos+1] = prediction\n",
    "            scores[beam_idx] = score_cache[max_idx]\n",
    "            score_cache[max_idx] = -1\n",
    "\n",
    "            if prediction[-1] == tgt_tokenizer.word_index['<end>']:\n",
    "                eos_flag[beam_idx] = -1\n",
    "\n",
    "    pred = []\n",
    "    for long_pred in pred_tmp:\n",
    "        zero_idx = long_pred.tolist().index(tgt_tokenizer.word_index['<end>'])\n",
    "        short_pred = long_pred[:zero_idx+1]\n",
    "        pred.append(short_pred)\n",
    "        \n",
    "    return  [' '.join([src_tokenizer.index_word[j] for j in i]) for i in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5e2306a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quesions 1: 지루하다, 놀러가고 싶어.\n",
      "response 1: ['<start> 마음 이 가까운 곳 하 는 마음 을 가 보 세요 . <end>', '<start> 마음 을 가까운 곳 하 는 마음 을 가 보 세요 . <end>', '<start> 마음 이 가까운 곳 하 나 마음 을 가 보 세요 . <end>']\n",
      "\n",
      "quesions 2: 오늘 일찍 일어났더니 피곤하다.\n",
      "response 2: ['<start> 오늘 도 즐거운 하루 보내 세요 . <end>', '<start> 오늘 도 즐거운 고생 보내 세요 . <end>', '<start> 오늘 도 즐거운 하루 보내 좋 . <end>']\n",
      "\n",
      "quesions 3: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "response 3: ['<start> 좋 은 날 이 죠 . <end>', '<start> 고백 은 날 이 죠 . <end>', '<start> 인연 은 날 이 죠 . <end>']\n",
      "\n",
      "quesions 4: 집에 있는다는 소리야.\n",
      "response 4: ['<start> 마련 이 편하 겠 네요 . <end>', '<start> 마련 이 편하 겠 죠 . <end>', '<start> 같이 이 편하 겠 네요 . <end>']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN= 50\n",
    "\n",
    "for i, example in enumerate(examples) :\n",
    "    print(f'quesions {i+1}:',example)\n",
    "    print(f'response {i+1}:', beam_search_decoder(example,\n",
    "                        MAX_LEN,\n",
    "                        MAX_LEN,\n",
    "                        transformer,\n",
    "                        tokenizer,\n",
    "                        tokenizer,\n",
    "                        beam_size=3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262a5e8e",
   "metadata": {},
   "source": [
    "### BLEU Single Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c190f8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def eval_bleu_single(model, src_sentence, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "    src_tokens = tokenizer.texts_to_sequences(src_sentence)\n",
    "    tgt_tokens = tokenizer.texts_to_sequences(tgt_sentence)\n",
    "\n",
    "    if (len(src_tokens) > MAX_LEN): return None\n",
    "    if (len(tgt_tokens) > MAX_LEN): return None\n",
    "\n",
    "    reference = tgt_sentence.split()[1:-1]\n",
    "    candidate = translate(src_sentence, model, src_tokenizer, tgt_tokenizer).split()\n",
    "\n",
    "    score = sentence_bleu(reference, candidate,\n",
    "                          smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Source Sentence: \", src_sentence)\n",
    "        print(\"Model Prediction: \", candidate)\n",
    "        print(\"Real: \", reference)\n",
    "        print(\"Score: %lf\\n\" % score)\n",
    "        \n",
    "    return score\n",
    "        \n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "29bd04be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  나 기재 됐 나 봐 \n",
      "Model Prediction:  ['모두', '가', '사랑', '앞', '에', '어리석', '게', '아니', '라', '생각', '해요', '.']\n",
      "Real:  ['확인', '해', '달', '라고', '해', '보', '세요', '.']\n",
      "Score: 0.020256\n",
      "\n",
      "Source Sentence:  공허 함 이 크 네\n",
      "Model Prediction:  ['제', '놀드', '채워', '줄게요', '.']\n",
      "Real:  ['저', '가', '채워', '줄게요', '.']\n",
      "Score: 0.053728\n",
      "\n",
      "Source Sentence:  일 하 는 곳 근처 가 서 염탐 하 고 왔 네 .\n",
      "Model Prediction:  ['하루', '일', '수', '밖', '에', '없', '군요', '.']\n",
      "Real:  ['그분', '에', '대일본', '궁금증', '을', '접', '어', '두', '셔요', '.']\n",
      "Score: 0.036556\n",
      "\n",
      "Source Sentence:  제 가 놓 았 던 손 을 다시 잡 으려니 힘드 네 .\n",
      "Model Prediction:  ['마음', '이', '서로', '마음', '이', '안', '좋', '겠', '어요', '.']\n",
      "Real:  ['원상', '복구', '는', '언제나', '힘드', '니까요', '.']\n",
      "Score: 0.021105\n",
      "\n",
      "Source Sentence:  이 나이 에 모르 는 게 왜 이렇게 많 지\n",
      "Model Prediction:  ['나', '이', '아쉽', '네요', '.']\n",
      "Real:  ['당연', '한', '것', '예요', '.']\n",
      "Score: 0.053728\n",
      "\n",
      "Source Sentence:  학교 에서 지속 잤 어 \n",
      "Model Prediction:  ['선생', '님', '이', '요', '.']\n",
      "Real:  ['한창', '잠', '이', '많', '을', '때', '죠', '.']\n",
      "Score: 0.063894\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 인덱스를 바꿔가며 테스트해 보세요\n",
    "test_idx = [0, 3, 7, 23, 59, 173]\n",
    "\n",
    "for i in test_idx:\n",
    "    eval_bleu_single(transformer, \n",
    "                 que_test[i], \n",
    "                 ans_test[i], \n",
    "                 tokenizer, \n",
    "                 tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b73d578",
   "metadata": {},
   "source": [
    "### BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c1f74ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def eval_bleu(model, src_sentences, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(src_sentences)\n",
    "    \n",
    "    for idx in tqdm(range(sample_size)):\n",
    "        score = eval_bleu_single(model, src_sentences[idx], tgt_sentence[idx], src_tokenizer, tgt_tokenizer, verbose)\n",
    "        if not score: continue\n",
    "        \n",
    "        total_score += score\n",
    "    \n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)\n",
    "    \n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149dcac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f03488df234938bc5aa985feb3fa1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3211 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_bleu(transformer, que_test, ans_test, tokenizer, tokenizer, verbose=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "241.59375px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
