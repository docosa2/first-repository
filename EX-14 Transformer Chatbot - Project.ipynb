{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e79685f",
   "metadata": {},
   "source": [
    "# EX-14 Chatbot - Project\n",
    "\n",
    "### 2023-01-04 (수)\n",
    "\n",
    "### 곽상혁\n",
    "\n",
    "https:/github.com/docosa2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e848e33",
   "metadata": {},
   "source": [
    "### 모듈 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d9278881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "bec0d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31901834",
   "metadata": {},
   "source": [
    "## 데이타 수집\n",
    "---\n",
    "\n",
    "한국어 챗봇 데이터는 송영숙님이 공개한 챗봇 데이터를 사용합니다.\n",
    "\n",
    "이 데이터는 아래의 링크에서 다운로드할 수 있습니다.\n",
    "\n",
    "* [songys/Chatbot_data](https://github.com/songys/Chatbot_data/blob/master/ChatbotData.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21d2a90",
   "metadata": {},
   "source": [
    "## 데이타 전처리\n",
    "\n",
    "영어 데이터와는 전혀 다른 데이터인 만큼 영어 데이터에 사용했던 전처리와 일부 동일한 전처리도 필요하겠지만 전체적으로는 다른 전처리를 수행해야 할 수도 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa43e9b",
   "metadata": {},
   "source": [
    "### 데이타 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "511fc972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./data/ChatbotData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b784f0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d3397e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11823, 3)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454a789c",
   "metadata": {},
   "source": [
    "### 데이타 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a6c8be3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence: str) -> str:    \n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d31cede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_entire_text_for(df: pd.DataFrame) -> None:\n",
    "    for index, row in df.iterrows():        \n",
    "        df.iloc[index, 0] = preprocess_sentence(row['Q'])\n",
    "        df.iloc[index, 1] = preprocess_sentence(row['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7e84b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_entire_text_for(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "77c7b59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡 !</td>\n",
       "      <td>하루가 또 가네요 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q              A  label\n",
       "0         12시 땡 !    하루가 또 가네요 .       0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다 .       0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠 .       0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠 .       0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠 .       0"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5296aa4a",
   "metadata": {},
   "source": [
    "### 데이타 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3508ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = data.Q.values.tolist()\n",
    "answers = data.A.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a0fbd70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9458\n",
      "9458\n",
      "\n",
      "2365\n",
      "2365\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(questions, answers, test_size=0.2, random_state=42)\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print()\n",
    "print(len(X_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea4133d",
   "metadata": {},
   "source": [
    "## SubwordTextEncoder 사용하기\n",
    "---\n",
    "\n",
    "한국어 데이터는 형태소 분석기를 사용하여 토크나이징을 해야 한다고 많은 분이 알고 있습니다. 하지만 여기서는 형태소 분석기가 아닌 위 실습에서 사용했던 내부 단어 토크나이저인 SubwordTextEncoder를 그대로 사용해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "96d12aa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(X_train + y_train, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "27572ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8730\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.vocab_size)\n",
    "SOS = [tokenizer.vocab_size]\n",
    "EOS = [tokenizer.vocab_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "27593345",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8732\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "866fdd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q : 엄청 로맨틱해\n",
      "Q : [417, 7718]\n",
      "\n",
      "A: 생각만해도 달콤하네요 . \n",
      "A: [417, 7718]\n"
     ]
    }
   ],
   "source": [
    "print('Q : {}'.format(X_train[0]))\n",
    "print('Q : {}'.format(tokenizer.encode(X_train[0])))\n",
    "print()\n",
    "print('A: {}'.format(y_train[0]))\n",
    "print('A: {}'.format(tokenizer.encode(X_train[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8fbfe902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max question length : 58\n",
      "max answer length: 79\n"
     ]
    }
   ],
   "source": [
    "print('max question length :', len(max(X_train, key=len)))\n",
    "print('max answer length:', len(max(y_train, key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "14cf1f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b12d4fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Add_Tokens_for_Start_and_End_of(sentence: str) -> str:\n",
    "    return SOS + tokenizer.encode(sentence) + EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "9b5a1207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pad_sequence(sentences: list[str], maxlen, padding='post') -> list[str]:\n",
    "    return tf.keras.preprocessing.sequence.pad_sequences(sentences, maxlen=maxlen, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "18b90f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩, 패딩\n",
    "def tokenize_and_filter(questions: list[str], answers: list[str]) -> (list[str], list[str]):\n",
    "    tokenized_questions = []\n",
    "    tokenized_answers = []\n",
    "  \n",
    "    for (question, answer) in zip(questions, answers):\n",
    "        # MAX_SENTENCE_LENGTH 인 데이타만 처리\n",
    "        if len(question) > MAX_SENTENCE_LENGTH or len(answer) > MAX_SENTENCE_LENGTH:        \n",
    "            continue        \n",
    "        \n",
    "        # 시작,종료 토큰 추가\n",
    "        tokenized_question = Add_Tokens_for_Start_and_End_of(question)\n",
    "        tokenized_answer = Add_Tokens_for_Start_and_End_of(answer)\n",
    "        \n",
    "        # append\n",
    "        tokenized_questions.append(tokenized_question)\n",
    "        tokenized_answers.append(tokenized_answer)\n",
    "  \n",
    "    # MAX_SENTENCE_LENGTH로 문장 패딩\n",
    "    tokenized_questions = Pad_sequence(tokenized_questions, MAX_SENTENCE_LENGTH)   \n",
    "    tokenized_answers = Pad_sequence(tokenized_answers, MAX_SENTENCE_LENGTH)\n",
    "  \n",
    "    return tokenized_questions, tokenized_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "06825282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 8732\n",
      "전처리된 질문수: 9425\n",
      "전처리된 답변수: 9425\n"
     ]
    }
   ],
   "source": [
    "questions, answers = tokenize_and_filter(X_train, y_train)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('전처리된 질문수: {}'.format(len(questions)))\n",
    "print('전처리된 답변수: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd0449c",
   "metadata": {},
   "source": [
    "## 모델 구성하기\n",
    "---\n",
    "\n",
    "위 실습 내용을 참고하여 트랜스포머 모델을 구현합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dbc22f",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0032ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.encode(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def encode(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "        \n",
    "        sin = tf.math.sin(angle_rads[:, 0::2]) # 배열의 짝수 인덱스에는 sin 함수 적용        \n",
    "        cos = tf.math.cos(angle_rads[:, 1::2]) # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "\n",
    "        pos_encoding = tf.concat([sin, cos], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85384a7a",
   "metadata": {},
   "source": [
    "### 스케일드 닷 프로덕트 어텐션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8f403e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    \"\"\"어텐션 가중치를 계산. \"\"\"\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # add the mask to zero out padding tokens\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k)\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536ae598",
   "metadata": {},
   "source": [
    "### 멀티헤드 어텐션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b1c9d762",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    \n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # linear layers\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # 병렬 연산을 위한 머리를 여러 개 만듭니다.\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # 스케일드 닷-프로덕트 어텐션 함수\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다.\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))\n",
    "\n",
    "        # final linear layer\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2cb98d",
   "metadata": {},
   "source": [
    "### 패딩 마스크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "01cfb235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)    \n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, sequence length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455b63aa",
   "metadata": {},
   "source": [
    "### Look Ahead 마스크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "aa610426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x) # 패딩마스크도 별도로 함께 적용해 준다\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f228a5",
   "metadata": {},
   "source": [
    "### 인코더 레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1fd36a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "    # 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 첫번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention = MultiHeadAttention(\n",
    "          d_model, num_heads, name=\"attention\")({\n",
    "              'query': inputs,\n",
    "              'key': inputs,\n",
    "              'value': inputs,\n",
    "              'mask': padding_mask\n",
    "          })\n",
    "\n",
    "    # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(\n",
    "          epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "    # 두번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "          epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "          inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d232c2",
   "metadata": {},
   "source": [
    "### 인코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "bb52ea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "    # 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    # num_layers만큼 쌓아올린 인코더의 층.\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"encoder_layer_{}\".format(i),\n",
    "        )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1826bc",
   "metadata": {},
   "source": [
    "### 디코더 레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "973a6a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "          shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 첫번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention1 = MultiHeadAttention(\n",
    "          d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "              'query': inputs,\n",
    "              'key': inputs,\n",
    "              'value': inputs,\n",
    "              'mask': look_ahead_mask\n",
    "          })\n",
    "\n",
    "    # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "          epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    # 두번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "    attention2 = MultiHeadAttention(\n",
    "          d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "              'query': attention1,\n",
    "              'key': enc_outputs,\n",
    "              'value': enc_outputs,\n",
    "              'mask': padding_mask\n",
    "          })\n",
    "\n",
    "    # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "    # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "          epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "    # 세번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "          epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "          inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "          outputs=outputs,\n",
    "          name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1872b248",
   "metadata": {},
   "source": [
    "### 디코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6027f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "    # 패딩 마스크\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c930427",
   "metadata": {},
   "source": [
    "### 트랜스포머"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f09c2552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    # 인코더에서 패딩을 위한 마스크\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "\n",
    "    # 디코더에서 미래의 토큰을 마스크하기위해서 사용합니다.\n",
    "    # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "    # 두번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
    "    # 디코더에서 패딩을 위한 마스크\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "    # 인코더\n",
    "    enc_outputs = encoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "    )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "    # 디코더\n",
    "    dec_outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    # 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1436b8f",
   "metadata": {},
   "source": [
    "### 교사 강요(Teacher Forcing) 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "dbea442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
    "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e53de60",
   "metadata": {},
   "source": [
    "## 모델 구성 및 하이퍼 파라미터 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9d7c75",
   "metadata": {},
   "source": [
    "### 하이퍼 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a5392293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f58aad",
   "metadata": {},
   "source": [
    "### 모델 인스턴스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "57c29ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 256)    3289600     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    3816960     dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8732)   2244124     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,350,684\n",
      "Trainable params: 9,350,684\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcff6c58",
   "metadata": {},
   "source": [
    "### 손실함수\n",
    "\n",
    "레이블인 시퀀스에 패딩이 되어 있으므로, loss를 계산할 때 패딩 마스크를 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "738e0b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_SENTENCE_LENGTH - 1))\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a0335c",
   "metadata": {},
   "source": [
    "### 커스텀 학습률 스케줄링(Custom Learning rate Scheduling) 적용\n",
    "\n",
    "딥러닝 모델학습 시 learning rate는 매우 중요한 하이퍼파라미터입니다. 최근에는 모델학습 초기에 learning rate를 급격히 높였다가, 이후 train step이 진행됨에 따라 서서히 낮추어 가면서 안정적으로 수렴하게 하는 고급 기법을 널리 사용하고 있습니다. 이런 방법을 커스텀 학습률 스케줄링(Custom Learning rate Scheduling)이라고 합니다.\n",
    "\n",
    "논문에 나온 공식을 참고하여 커스텀 학습률 스케줄러를 통한 아담 옵티마이저를 사용합니다. 논문에 나온 공식은 다음과 같습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "4f752d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0832b99d",
   "metadata": {},
   "source": [
    "#### 학습률 스케쥴링 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "79003edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_learning_rate = CustomSchedule(d_model=128)\n",
    "\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f92d05f",
   "metadata": {},
   "source": [
    "### 모델 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "26403a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_SENTENCE_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8c547d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb4aa64",
   "metadata": {},
   "source": [
    "## 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "50e45fc0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "148/148 [==============================] - 15s 66ms/step - loss: 0.8159 - accuracy: 0.0378\n",
      "Epoch 2/200\n",
      "148/148 [==============================] - 10s 65ms/step - loss: 0.7069 - accuracy: 0.0433\n",
      "Epoch 3/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.6567 - accuracy: 0.0465\n",
      "Epoch 4/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.5753 - accuracy: 0.0523\n",
      "Epoch 5/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.4653 - accuracy: 0.0637\n",
      "Epoch 6/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.3653 - accuracy: 0.0752\n",
      "Epoch 7/200\n",
      "148/148 [==============================] - 10s 67ms/step - loss: 0.2940 - accuracy: 0.0839\n",
      "Epoch 8/200\n",
      "148/148 [==============================] - 10s 67ms/step - loss: 0.2462 - accuracy: 0.0913\n",
      "Epoch 9/200\n",
      "148/148 [==============================] - 10s 67ms/step - loss: 0.2126 - accuracy: 0.0968\n",
      "Epoch 10/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.1852 - accuracy: 0.1007\n",
      "Epoch 11/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.1652 - accuracy: 0.1037\n",
      "Epoch 12/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.1480 - accuracy: 0.1058\n",
      "Epoch 13/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.1330 - accuracy: 0.1080\n",
      "Epoch 14/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.1196 - accuracy: 0.1097\n",
      "Epoch 15/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.1071 - accuracy: 0.1116\n",
      "Epoch 16/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0973 - accuracy: 0.1133\n",
      "Epoch 17/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0890 - accuracy: 0.1146\n",
      "Epoch 18/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0796 - accuracy: 0.1162\n",
      "Epoch 19/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0727 - accuracy: 0.1175\n",
      "Epoch 20/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0650 - accuracy: 0.1191\n",
      "Epoch 21/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0585 - accuracy: 0.1207\n",
      "Epoch 22/200\n",
      "148/148 [==============================] - 10s 67ms/step - loss: 0.0546 - accuracy: 0.1214\n",
      "Epoch 23/200\n",
      "148/148 [==============================] - 10s 67ms/step - loss: 0.0517 - accuracy: 0.1220\n",
      "Epoch 24/200\n",
      "148/148 [==============================] - 10s 67ms/step - loss: 0.0483 - accuracy: 0.1228\n",
      "Epoch 25/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0438 - accuracy: 0.1239\n",
      "Epoch 26/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0396 - accuracy: 0.1250\n",
      "Epoch 27/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0360 - accuracy: 0.1259\n",
      "Epoch 28/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0335 - accuracy: 0.1266\n",
      "Epoch 29/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0330 - accuracy: 0.1268\n",
      "Epoch 30/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0324 - accuracy: 0.1269\n",
      "Epoch 31/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0324 - accuracy: 0.1267\n",
      "Epoch 32/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0293 - accuracy: 0.1277\n",
      "Epoch 33/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0286 - accuracy: 0.1279\n",
      "Epoch 34/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0266 - accuracy: 0.1286\n",
      "Epoch 35/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0272 - accuracy: 0.1285\n",
      "Epoch 36/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0274 - accuracy: 0.1282\n",
      "Epoch 37/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0251 - accuracy: 0.1289\n",
      "Epoch 38/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0241 - accuracy: 0.1293\n",
      "Epoch 39/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0246 - accuracy: 0.1289\n",
      "Epoch 40/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0259 - accuracy: 0.1288\n",
      "Epoch 41/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0233 - accuracy: 0.1294\n",
      "Epoch 42/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0236 - accuracy: 0.1294\n",
      "Epoch 43/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0218 - accuracy: 0.1298\n",
      "Epoch 44/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0216 - accuracy: 0.1298\n",
      "Epoch 45/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0212 - accuracy: 0.1301\n",
      "Epoch 46/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0197 - accuracy: 0.1304\n",
      "Epoch 47/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0216 - accuracy: 0.1299\n",
      "Epoch 48/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0207 - accuracy: 0.1300\n",
      "Epoch 49/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0200 - accuracy: 0.1303\n",
      "Epoch 50/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0205 - accuracy: 0.1304\n",
      "Epoch 51/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0204 - accuracy: 0.1302\n",
      "Epoch 52/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0208 - accuracy: 0.1301\n",
      "Epoch 53/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0190 - accuracy: 0.1306\n",
      "Epoch 54/200\n",
      "148/148 [==============================] - 10s 67ms/step - loss: 0.0194 - accuracy: 0.1304\n",
      "Epoch 55/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0184 - accuracy: 0.1308\n",
      "Epoch 56/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0187 - accuracy: 0.1306\n",
      "Epoch 57/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0190 - accuracy: 0.1305\n",
      "Epoch 58/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0192 - accuracy: 0.1305\n",
      "Epoch 59/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0199 - accuracy: 0.1303\n",
      "Epoch 60/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0187 - accuracy: 0.1306\n",
      "Epoch 61/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0177 - accuracy: 0.1309\n",
      "Epoch 62/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0168 - accuracy: 0.1312\n",
      "Epoch 63/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0163 - accuracy: 0.1313\n",
      "Epoch 64/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0169 - accuracy: 0.1312\n",
      "Epoch 65/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0163 - accuracy: 0.1315\n",
      "Epoch 66/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0167 - accuracy: 0.1312\n",
      "Epoch 67/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0163 - accuracy: 0.1314\n",
      "Epoch 68/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0160 - accuracy: 0.1314\n",
      "Epoch 69/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0168 - accuracy: 0.1311\n",
      "Epoch 70/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0164 - accuracy: 0.1314\n",
      "Epoch 71/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0173 - accuracy: 0.1311\n",
      "Epoch 72/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0164 - accuracy: 0.1313\n",
      "Epoch 73/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0161 - accuracy: 0.1313\n",
      "Epoch 74/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0159 - accuracy: 0.1314\n",
      "Epoch 75/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0149 - accuracy: 0.1317\n",
      "Epoch 76/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0139 - accuracy: 0.1319\n",
      "Epoch 77/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0131 - accuracy: 0.1323\n",
      "Epoch 78/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0143 - accuracy: 0.1320\n",
      "Epoch 79/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0150 - accuracy: 0.1318\n",
      "Epoch 80/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0152 - accuracy: 0.1316\n",
      "Epoch 81/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0144 - accuracy: 0.1319\n",
      "Epoch 82/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0150 - accuracy: 0.1317\n",
      "Epoch 83/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0153 - accuracy: 0.1316\n",
      "Epoch 84/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0146 - accuracy: 0.1318\n",
      "Epoch 85/200\n",
      "148/148 [==============================] - 10s 67ms/step - loss: 0.0143 - accuracy: 0.1319\n",
      "Epoch 86/200\n",
      "148/148 [==============================] - 10s 67ms/step - loss: 0.0138 - accuracy: 0.1320\n",
      "Epoch 87/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0139 - accuracy: 0.1321\n",
      "Epoch 88/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0140 - accuracy: 0.1322\n",
      "Epoch 89/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0136 - accuracy: 0.1321\n",
      "Epoch 90/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0130 - accuracy: 0.1323\n",
      "Epoch 91/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0133 - accuracy: 0.1322\n",
      "Epoch 92/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0132 - accuracy: 0.1322\n",
      "Epoch 93/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0135 - accuracy: 0.1321\n",
      "Epoch 94/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0139 - accuracy: 0.1319\n",
      "Epoch 95/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0137 - accuracy: 0.1321\n",
      "Epoch 96/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0129 - accuracy: 0.1323\n",
      "Epoch 97/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0131 - accuracy: 0.1322\n",
      "Epoch 98/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0141 - accuracy: 0.1320\n",
      "Epoch 99/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0128 - accuracy: 0.1324\n",
      "Epoch 100/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0120 - accuracy: 0.1325\n",
      "Epoch 101/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0120 - accuracy: 0.1325\n",
      "Epoch 102/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0124 - accuracy: 0.1324\n",
      "Epoch 103/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0120 - accuracy: 0.1325\n",
      "Epoch 104/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0128 - accuracy: 0.1323\n",
      "Epoch 105/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0121 - accuracy: 0.1325\n",
      "Epoch 106/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0125 - accuracy: 0.1325\n",
      "Epoch 107/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0123 - accuracy: 0.1324\n",
      "Epoch 108/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0130 - accuracy: 0.1324\n",
      "Epoch 109/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0134 - accuracy: 0.1323\n",
      "Epoch 110/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0108 - accuracy: 0.1328\n",
      "Epoch 111/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0112 - accuracy: 0.1329\n",
      "Epoch 112/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0116 - accuracy: 0.1327\n",
      "Epoch 113/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0113 - accuracy: 0.1327\n",
      "Epoch 114/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0111 - accuracy: 0.1328\n",
      "Epoch 115/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0122 - accuracy: 0.1325\n",
      "Epoch 116/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0122 - accuracy: 0.1325\n",
      "Epoch 117/200\n",
      "148/148 [==============================] - 10s 67ms/step - loss: 0.0120 - accuracy: 0.1325\n",
      "Epoch 118/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0118 - accuracy: 0.1327\n",
      "Epoch 119/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0126 - accuracy: 0.1324\n",
      "Epoch 120/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0121 - accuracy: 0.1325\n",
      "Epoch 121/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0117 - accuracy: 0.1325\n",
      "Epoch 122/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0111 - accuracy: 0.1328\n",
      "Epoch 123/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0117 - accuracy: 0.1327\n",
      "Epoch 124/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0112 - accuracy: 0.1328\n",
      "Epoch 125/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0109 - accuracy: 0.1330\n",
      "Epoch 126/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0106 - accuracy: 0.1329\n",
      "Epoch 127/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0104 - accuracy: 0.1330\n",
      "Epoch 128/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0104 - accuracy: 0.1329\n",
      "Epoch 129/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0102 - accuracy: 0.1330\n",
      "Epoch 130/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0105 - accuracy: 0.1330\n",
      "Epoch 131/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0109 - accuracy: 0.1329\n",
      "Epoch 132/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0102 - accuracy: 0.1330\n",
      "Epoch 133/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0105 - accuracy: 0.1330\n",
      "Epoch 134/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0095 - accuracy: 0.1333\n",
      "Epoch 135/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0103 - accuracy: 0.1330\n",
      "Epoch 136/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0104 - accuracy: 0.1331\n",
      "Epoch 137/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0101 - accuracy: 0.1330\n",
      "Epoch 138/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0108 - accuracy: 0.1329\n",
      "Epoch 139/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0095 - accuracy: 0.1331\n",
      "Epoch 140/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0097 - accuracy: 0.1332\n",
      "Epoch 141/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0108 - accuracy: 0.1327\n",
      "Epoch 142/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0103 - accuracy: 0.1330\n",
      "Epoch 143/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0098 - accuracy: 0.1332\n",
      "Epoch 144/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0095 - accuracy: 0.1333\n",
      "Epoch 145/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0099 - accuracy: 0.1332\n",
      "Epoch 146/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0097 - accuracy: 0.1333\n",
      "Epoch 147/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0093 - accuracy: 0.1333\n",
      "Epoch 148/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0093 - accuracy: 0.1333\n",
      "Epoch 149/200\n",
      "148/148 [==============================] - 10s 67ms/step - loss: 0.0093 - accuracy: 0.1333\n",
      "Epoch 150/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0105 - accuracy: 0.1329\n",
      "Epoch 151/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0103 - accuracy: 0.1331\n",
      "Epoch 152/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0102 - accuracy: 0.1331\n",
      "Epoch 153/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0097 - accuracy: 0.1332\n",
      "Epoch 154/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0105 - accuracy: 0.1329\n",
      "Epoch 155/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0099 - accuracy: 0.1331\n",
      "Epoch 156/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0111 - accuracy: 0.1328\n",
      "Epoch 157/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0098 - accuracy: 0.1331\n",
      "Epoch 158/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0096 - accuracy: 0.1331\n",
      "Epoch 159/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0100 - accuracy: 0.1331\n",
      "Epoch 160/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0097 - accuracy: 0.1332\n",
      "Epoch 161/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0091 - accuracy: 0.1333\n",
      "Epoch 162/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0089 - accuracy: 0.1334\n",
      "Epoch 163/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0089 - accuracy: 0.1333\n",
      "Epoch 164/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0091 - accuracy: 0.1333\n",
      "Epoch 165/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0088 - accuracy: 0.1334\n",
      "Epoch 166/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0083 - accuracy: 0.1336\n",
      "Epoch 167/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0084 - accuracy: 0.1335\n",
      "Epoch 168/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0083 - accuracy: 0.1336\n",
      "Epoch 169/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0082 - accuracy: 0.1336\n",
      "Epoch 170/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0085 - accuracy: 0.1334\n",
      "Epoch 171/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0089 - accuracy: 0.1335\n",
      "Epoch 172/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0098 - accuracy: 0.1332\n",
      "Epoch 173/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0091 - accuracy: 0.1333\n",
      "Epoch 174/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0085 - accuracy: 0.1336\n",
      "Epoch 175/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0089 - accuracy: 0.1334\n",
      "Epoch 176/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0085 - accuracy: 0.1335\n",
      "Epoch 177/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0088 - accuracy: 0.1334\n",
      "Epoch 178/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0085 - accuracy: 0.1334\n",
      "Epoch 179/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0088 - accuracy: 0.1334\n",
      "Epoch 180/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0082 - accuracy: 0.13350s - loss: 0.0\n",
      "Epoch 181/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0090 - accuracy: 0.1334\n",
      "Epoch 182/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0094 - accuracy: 0.1332\n",
      "Epoch 183/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0088 - accuracy: 0.1334\n",
      "Epoch 184/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0083 - accuracy: 0.1336\n",
      "Epoch 185/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0083 - accuracy: 0.1336\n",
      "Epoch 186/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0082 - accuracy: 0.1336\n",
      "Epoch 187/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0079 - accuracy: 0.1336\n",
      "Epoch 188/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0080 - accuracy: 0.1336\n",
      "Epoch 189/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0079 - accuracy: 0.1336\n",
      "Epoch 190/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0079 - accuracy: 0.1337\n",
      "Epoch 191/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0079 - accuracy: 0.1337\n",
      "Epoch 192/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0083 - accuracy: 0.1336\n",
      "Epoch 193/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0087 - accuracy: 0.1335\n",
      "Epoch 194/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0078 - accuracy: 0.1336\n",
      "Epoch 195/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0079 - accuracy: 0.1336\n",
      "Epoch 196/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0083 - accuracy: 0.1336\n",
      "Epoch 197/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0091 - accuracy: 0.1334\n",
      "Epoch 198/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0081 - accuracy: 0.1336\n",
      "Epoch 199/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0079 - accuracy: 0.1337\n",
      "Epoch 200/200\n",
      "148/148 [==============================] - 10s 66ms/step - loss: 0.0080 - accuracy: 0.1336\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "\n",
    "history = model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "165d8094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], 'r')\n",
    "plt.plot(history.history['accuracy'], 'b')\n",
    "plt.title('Model Accuracy ')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcca479",
   "metadata": {},
   "source": [
    "## 모델 평가하기\n",
    "---\n",
    "\n",
    "Step 1에서 선택한 전처리 방법을 고려하여 입력된 문장에 대해서 대답을 얻는 예측 함수를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f9412ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(sentence):\n",
    "    # sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    sentence = tf.expand_dims(\n",
    "      SOS + tokenizer.encode(sentence) + EOS, axis=0)\n",
    "\n",
    "    # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "    # 처음에는 예측한 내용이 없으므로 시작 토큰만 별도 저장. ex) 8331\n",
    "    output_sequence = tf.expand_dims(SOS, 0)\n",
    "\n",
    "    # 디코더의 인퍼런스 단계\n",
    "    for i in range(MAX_SENTENCE_LENGTH):\n",
    "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "        predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "\n",
    "        # 현재 예측한 단어의 정수\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "        if tf.equal(predicted_id, EOS[0]):\n",
    "            break\n",
    "\n",
    "        # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "        # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output_sequence, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "2e598ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(sentence):\n",
    "    # 입력 문장에 대해서 디코더를 동작시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "    prediction = decoder_inference(sentence)\n",
    "\n",
    "    # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ec2b55",
   "metadata": {},
   "source": [
    "### Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ea9793c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 입력 : 죽을거 같네\n",
      "답변 출력 : 힘내세요 . \n",
      "------------------------------\n",
      "질문 입력 : 내일 시험이야\n",
      "답변 출력 : 너무 집착하지 마세요 . \n",
      "------------------------------\n",
      "질문 입력 : 정말 . 내 자신이 싫다\n",
      "답변 출력 : 시작할 때는 그 끝을 알지 못하죠 . \n",
      "------------------------------\n",
      "질문 입력 : 이별후 네달째\n",
      "답변 출력 : 사람 마음은 알기 어렵죠 . \n",
      "------------------------------\n",
      "질문 입력 : 쌍커풀 해볼까\n",
      "답변 출력 : 도움을 요청하면 더 쉽게 가까워질 수 있을 거예요 . \n",
      "------------------------------\n",
      "질문 입력 : 내 생각 하나만 바꾸면 편할텐데 . \n",
      "답변 출력 : 따로 살아온 삶과 습관 , 성향이 다르기 때문일 거예요 . \n",
      "------------------------------\n",
      "질문 입력 : 어떻게 살아가야 할까\n",
      "답변 출력 : 그런 시기가 있더라고요 . \n",
      "------------------------------\n",
      "질문 입력 : 발 아파\n",
      "답변 출력 : 짧게 변화를 줘도 괜찮을 거 같아요 . \n",
      "------------------------------\n",
      "질문 입력 : 썸 타는 것도 귀찮아 . \n",
      "답변 출력 : 자주 생각하나요 . \n",
      "------------------------------\n",
      "질문 입력 : 좋아하는 애랑 전화하면\n",
      "답변 출력 : 특별한 경험이 될 것 같아요 . \n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('질문 입력 : {}'.format(X_test[i]))\n",
    "    print('답변 출력 : {}'.format(sentence_generation(X_test[i])))\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d501f1a0",
   "metadata": {},
   "source": [
    "### 고찰\n",
    "1. 트랜스포머의 구성 요소들을 단계적으로 작성하면서 내부 작동 방식과 전체적인 흐름에 대해 대략적으로 이해할 수 있었다.\n",
    "2. 학습을 실시했지만 loss의 감소에 비해 accuracy의 상승이 높지 않았다. 학습 데이타가 더 많으면 성능이 좀 더 개선될 것 같았다. 큰 규모의 대화 데이타 세트를 이용하여 다른 분야, 특히 의료 분야의 대화 챗봇에 응용해 보면 좋겠다는 생각을 했다.\n",
    "3. 포지셔널 인코딩의 개념이 바로 이해되지 않아 어려움을 겪었다. 여러 자료를 찾아보던 중 일러스트로 설명해 놓은 자료를 보고 이해할 수 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9314b1d5",
   "metadata": {},
   "source": [
    "### 참고자료\n",
    "\n",
    "1. What is Positional Encoding?\n",
    "https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model\n",
    "\n",
    "2. Transformer와 Nerf에서의 Positional Encoding의 의미와 사용 목적\n",
    "https://gaussian37.github.io/dl-concept-positional_encoding/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578d056d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "188.390625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
