{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b997d503",
   "metadata": {},
   "source": [
    "# EX-10 Translator seq2seq\n",
    "\n",
    "### 곽상혁\n",
    "\n",
    "### 2022-12-09 (금)\n",
    "\n",
    "https://github.com/docosa2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43816dc0",
   "metadata": {},
   "source": [
    "## 모듈 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "649463ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d99861a",
   "metadata": {},
   "source": [
    "실습에서 구현한 번역기는 글자 단위(Character-level)에서 구현된 번역기였습니다. 하지만 실제 번역기의 경우에는 글자 단위가 아니라 단어 단위(Word-level)에서 구현되는 것이 좀 더 보편적입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8551aa",
   "metadata": {},
   "source": [
    "동일한 데이터셋을 사용하면서 글자 단위와는 다른 전처리와 to_categorical() 함수가 아닌 임베딩 층(Embedding layer)를 추가하여 단어 단위의 번역기를 완성시켜보겠습니다. 하지만, 단어 단위로 할 경우에는 단어의 개수가 글자 단위로 했을 경우와 비교하여 단어장의 크기(Vocabulary) 크기도 커지고, 학습 속도도 좀 더 느려집니다. 학습과 테스트 시의 원활한 진행을 위해서 데이터에서 상위 33,000개의 샘플만 사용해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b64a2d",
   "metadata": {},
   "source": [
    "33000개 중 3000개는 테스트 데이터로 분리하여 모델을 학습한 후에 번역을 테스트 하는 용도로 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f0d2a3",
   "metadata": {},
   "source": [
    "### 데이타 파일 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "550b9c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 197463\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67377</th>\n",
       "      <td>Tom bought Mary a scarf.</td>\n",
       "      <td>Tom a acheté une écharpe pour Marie.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88376</th>\n",
       "      <td>I wish he were on our team.</td>\n",
       "      <td>J'aimerais qu'il fasse partie de notre équipe.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180825</th>\n",
       "      <td>I promised to help my brother with his homework.</td>\n",
       "      <td>J'ai promis d'aider mon frère avec ses devoirs.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34467</th>\n",
       "      <td>Bring them with you.</td>\n",
       "      <td>Apportez-les avec vous.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74433</th>\n",
       "      <td>That's not really my job.</td>\n",
       "      <td>Ce n'est pas vraiment mon travail.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     eng  \\\n",
       "67377                           Tom bought Mary a scarf.   \n",
       "88376                        I wish he were on our team.   \n",
       "180825  I promised to help my brother with his homework.   \n",
       "34467                               Bring them with you.   \n",
       "74433                          That's not really my job.   \n",
       "\n",
       "                                                    fra  \\\n",
       "67377              Tom a acheté une écharpe pour Marie.   \n",
       "88376    J'aimerais qu'il fasse partie de notre équipe.   \n",
       "180825  J'ai promis d'aider mon frère avec ses devoirs.   \n",
       "34467                           Apportez-les avec vous.   \n",
       "74433                Ce n'est pas vraiment mon travail.   \n",
       "\n",
       "                                                       cc  \n",
       "67377   CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
       "88376   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "180825  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "34467   CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "74433   CC-BY 2.0 (France) Attribution: tatoeba.org #4...  "
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5) #샘플 5개 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc72df6",
   "metadata": {},
   "source": [
    "### 중복 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "04b0839c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 제거 후 전체 샘플의 수 : 138912\n"
     ]
    }
   ],
   "source": [
    "lines = lines.drop_duplicates(subset='eng')\n",
    "print('중복 제거 후 전체 샘플의 수 :',len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c7de6",
   "metadata": {},
   "source": [
    "### 불필요한 칼럼 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "904fe148",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = lines[['eng', 'fra']][-36000:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "1370c7d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm the one who picked those flowers.</td>\n",
       "      <td>C'est moi-même qui ai cueilli ces fleurs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm the one who should be doing that.</td>\n",
       "      <td>Je suis celui qui devrait être en train de fai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm the one who should be doing this.</td>\n",
       "      <td>C'est moi qui devrais faire ceci.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm the only one planning to do that.</td>\n",
       "      <td>Je ne suis pas le seul à prévoir de le faire.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm the only one who understands Tom.</td>\n",
       "      <td>Je suis le seul qui comprend Tom.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35995</th>\n",
       "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
       "      <td>Une empreinte carbone est la somme de pollutio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35996</th>\n",
       "      <td>Death is something that we're often discourage...</td>\n",
       "      <td>La mort est une chose qu'on nous décourage sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35997</th>\n",
       "      <td>Since there are usually multiple websites on a...</td>\n",
       "      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35998</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35999</th>\n",
       "      <td>It may be impossible to get a completely error...</td>\n",
       "      <td>Il est peut-être impossible d'obtenir un Corpu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     eng  \\\n",
       "0                  I'm the one who picked those flowers.   \n",
       "1                  I'm the one who should be doing that.   \n",
       "2                  I'm the one who should be doing this.   \n",
       "3                  I'm the only one planning to do that.   \n",
       "4                  I'm the only one who understands Tom.   \n",
       "...                                                  ...   \n",
       "35995  A carbon footprint is the amount of carbon dio...   \n",
       "35996  Death is something that we're often discourage...   \n",
       "35997  Since there are usually multiple websites on a...   \n",
       "35998  If someone who doesn't know your background sa...   \n",
       "35999  It may be impossible to get a completely error...   \n",
       "\n",
       "                                                     fra  \n",
       "0              C'est moi-même qui ai cueilli ces fleurs.  \n",
       "1      Je suis celui qui devrait être en train de fai...  \n",
       "2                      C'est moi qui devrais faire ceci.  \n",
       "3          Je ne suis pas le seul à prévoir de le faire.  \n",
       "4                      Je suis le seul qui comprend Tom.  \n",
       "...                                                  ...  \n",
       "35995  Une empreinte carbone est la somme de pollutio...  \n",
       "35996  La mort est une chose qu'on nous décourage sou...  \n",
       "35997  Puisqu'il y a de multiples sites web sur chaqu...  \n",
       "35998  Si quelqu'un qui ne connaît pas vos antécédent...  \n",
       "35999  Il est peut-être impossible d'obtenir un Corpu...  \n",
       "\n",
       "[36000 rows x 2 columns]"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfef339",
   "metadata": {},
   "source": [
    "### 셔플"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "4f93fcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = lines.iloc[np.random.permutation(lines.index)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "d27c2c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apparently, the murder happened in a locked room.</td>\n",
       "      <td>Apparemment, le meurtre a eu lieu en chambre c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My cat likes to look through the window.</td>\n",
       "      <td>Mon chat aime regarder par la fenêtre.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is the dictionary I use every day.</td>\n",
       "      <td>C'est le dictionnaire que j'utilise tous les j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I want an hourly update about what's happening.</td>\n",
       "      <td>Je veux être tenu au courant de ce qui se pass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You'll get half the money now, and the other h...</td>\n",
       "      <td>Vous recevrez la moitié de l'argent maintenant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35995</th>\n",
       "      <td>Next time, ask me before you use my car.</td>\n",
       "      <td>La prochaine fois, demande-moi avant de te ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35996</th>\n",
       "      <td>I don't think you did this by yourself.</td>\n",
       "      <td>Je ne pense pas que tu aies fait ceci par toi-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35997</th>\n",
       "      <td>You haven't seen Tom today, have you?</td>\n",
       "      <td>Vous n'avez pas vu Tom aujourd'hui, n'est-ce p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35998</th>\n",
       "      <td>She was the most beautiful woman he had ever s...</td>\n",
       "      <td>Elle était la plus belle femme qu'il avait jam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35999</th>\n",
       "      <td>Tom doesn't have any reason to be jealous.</td>\n",
       "      <td>Tom n'a aucune raison d'être jaloux.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     eng  \\\n",
       "0      Apparently, the murder happened in a locked room.   \n",
       "1               My cat likes to look through the window.   \n",
       "2                This is the dictionary I use every day.   \n",
       "3        I want an hourly update about what's happening.   \n",
       "4      You'll get half the money now, and the other h...   \n",
       "...                                                  ...   \n",
       "35995           Next time, ask me before you use my car.   \n",
       "35996            I don't think you did this by yourself.   \n",
       "35997              You haven't seen Tom today, have you?   \n",
       "35998  She was the most beautiful woman he had ever s...   \n",
       "35999         Tom doesn't have any reason to be jealous.   \n",
       "\n",
       "                                                     fra  \n",
       "0      Apparemment, le meurtre a eu lieu en chambre c...  \n",
       "1                 Mon chat aime regarder par la fenêtre.  \n",
       "2      C'est le dictionnaire que j'utilise tous les j...  \n",
       "3      Je veux être tenu au courant de ce qui se pass...  \n",
       "4      Vous recevrez la moitié de l'argent maintenant...  \n",
       "...                                                  ...  \n",
       "35995  La prochaine fois, demande-moi avant de te ser...  \n",
       "35996  Je ne pense pas que tu aies fait ceci par toi-...  \n",
       "35997  Vous n'avez pas vu Tom aujourd'hui, n'est-ce p...  \n",
       "35998  Elle était la plus belle femme qu'il avait jam...  \n",
       "35999               Tom n'a aucune raison d'être jaloux.  \n",
       "\n",
       "[36000 rows x 2 columns]"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aabf357",
   "metadata": {},
   "source": [
    "### Step 1. 정제, 정규화, 전처리 (영어, 프랑스어 모두!)\n",
    "---\n",
    "\n",
    "글자 단위가 아닌 단어 단위의 번역기를 하기 위해서는 글자 단위에서는 신경쓰지 않았던 몇 가지 추가적인 전처리가 필요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ce552c",
   "metadata": {},
   "source": [
    "1. 구두점(Punctuation)을 단어와 분리해주세요.\n",
    "\n",
    "일반적으로 영어권 언어의 경우에는 띄어쓰기 단위로 단어를 분리합니다. 토큰화(Tokenization) 라고도 불리는 이 작업은 어디서부터 어디까지가 하나의 단어인지를 구분하는 작업인데요, 그런데 띄어쓰기를 해주기 전에 구두점을 분리하는 작업이 필요할 때가 있습니다.\n",
    "예를 들어서 'he is a good boy!'라는 문장이 있을 때, 이를 띄어쓰기 단위로 토큰화한다면 ['he', 'is', 'a', 'good', 'boy!']가 됩니다. 그런데 실제로 !는 boy와 붙어있는 한 단어가 아니므로 좀 더 올바른 전처리는 ['he', 'is', 'a', 'good', 'boy', '!']가 맞습니다.\n",
    "!나 ? 또는 온점과 같은 특수문자들을 구두점(punctuation)이라고 부릅니다. 이들을 토큰화하기 전에 단어와 미리 분리시켜주세요!\n",
    "\n",
    "> 분리 전 : he is a Good boy!  \n",
    "> 분리 후 : he is a Good boy !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "d0563861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def Seperate_punctuations(sentence: str) -> str:\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "ebe5b28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Seperate_punctuations('This Is, So To Speak, A Kind Of A Test Sentence.') == ('This Is ,  So To Speak ,  A Kind Of A Test Sentence . '), 'Error in converter to seperate punctuations.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "9571186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sepearate_entire_punctuations_for(df: pd.DataFrame) -> None:\n",
    "    for index, row in df.iterrows():\n",
    "        row['eng'] = Seperate_punctuations(row['eng'])\n",
    "        row['fra'] = Seperate_punctuations(row['fra'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "1dab3479",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sepearate_entire_punctuations_for(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "9498adb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apparently ,  the murder happened in a locked ...</td>\n",
       "      <td>Apparemment ,  le meurtre a eu lieu en chambre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My cat likes to look through the window .</td>\n",
       "      <td>Mon chat aime regarder par la fenêtre .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is the dictionary I use every day .</td>\n",
       "      <td>C'est le dictionnaire que j'utilise tous les j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I want an hourly update about what's happening .</td>\n",
       "      <td>Je veux être tenu au courant de ce qui se pass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You'll get half the money now ,  and the other...</td>\n",
       "      <td>Vous recevrez la moitié de l'argent maintenant...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 eng  \\\n",
       "0  Apparently ,  the murder happened in a locked ...   \n",
       "1         My cat likes to look through the window .    \n",
       "2          This is the dictionary I use every day .    \n",
       "3  I want an hourly update about what's happening .    \n",
       "4  You'll get half the money now ,  and the other...   \n",
       "\n",
       "                                                 fra  \n",
       "0  Apparemment ,  le meurtre a eu lieu en chambre...  \n",
       "1           Mon chat aime regarder par la fenêtre .   \n",
       "2  C'est le dictionnaire que j'utilise tous les j...  \n",
       "3  Je veux être tenu au courant de ce qui se pass...  \n",
       "4  Vous recevrez la moitié de l'argent maintenant...  "
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0c0c1a",
   "metadata": {},
   "source": [
    "2. 소문자로 바꿔주세요.\n",
    "\n",
    "기계가 보기에는 스펠링이 같더라도 대문자로 된 단어와 소문자로 된 단어는 서로 다른 단어입니다. 예를 들어 'Good'과 'good'은 기계가 보기에는 다른 단어입니다. 그래서 모든 문장에 대해서 전부 영어로 바꿔주는 작업을 하겠습니다.\n",
    "\n",
    "> 변환 전 : he is a Good boy !  \n",
    "> 변환 후 : he is a good boy !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "19cd5ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def Convert_to_lower_case(sentence: str) -> str:\n",
    "    sentence = sentence.lower().strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "aa164c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Convert_to_lower_case('This Is A Test Sentence.') == ('this is a test sentence.'), 'Error in converter for lower case.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "ab2562fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert_to_lower_cases_for(df: pd.DataFrame) -> None:\n",
    "    for index, row in df.iterrows():\n",
    "        row['eng'] = Convert_to_lower_case(row['eng'])\n",
    "        row['fra'] = Convert_to_lower_case(row['fra'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "3c9203eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Convert_to_lower_cases_for(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "4b407bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apparently ,  the murder happened in a locked ...</td>\n",
       "      <td>apparemment ,  le meurtre a eu lieu en chambre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>my cat likes to look through the window .</td>\n",
       "      <td>mon chat aime regarder par la fenêtre .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is the dictionary i use every day .</td>\n",
       "      <td>c'est le dictionnaire que j'utilise tous les j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i want an hourly update about what's happening .</td>\n",
       "      <td>je veux être tenu au courant de ce qui se pass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you'll get half the money now ,  and the other...</td>\n",
       "      <td>vous recevrez la moitié de l'argent maintenant...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 eng  \\\n",
       "0  apparently ,  the murder happened in a locked ...   \n",
       "1          my cat likes to look through the window .   \n",
       "2           this is the dictionary i use every day .   \n",
       "3   i want an hourly update about what's happening .   \n",
       "4  you'll get half the money now ,  and the other...   \n",
       "\n",
       "                                                 fra  \n",
       "0  apparemment ,  le meurtre a eu lieu en chambre...  \n",
       "1            mon chat aime regarder par la fenêtre .  \n",
       "2  c'est le dictionnaire que j'utilise tous les j...  \n",
       "3  je veux être tenu au courant de ce qui se pass...  \n",
       "4  vous recevrez la moitié de l'argent maintenant...  "
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cecb8ef",
   "metadata": {},
   "source": [
    "### 디코더에 시작토큰과 종료 토큰 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "a9368f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 36000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33861</th>\n",
       "      <td>i make it a rule never to borrow money .</td>\n",
       "      <td>sostoken j'ai érigé en règle de ne jamais empr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23961</th>\n",
       "      <td>certain details of the crime were not made pub...</td>\n",
       "      <td>sostoken certains détails du crime n'ont pas é...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27491</th>\n",
       "      <td>the two pieces were glued tightly together .</td>\n",
       "      <td>sostoken les deux morceaux furent fermement co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35947</th>\n",
       "      <td>i need you to help me move this bookcase .</td>\n",
       "      <td>sostoken j'ai besoin que tu m'aides à déplacer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17084</th>\n",
       "      <td>this isn't the first time this has happened .</td>\n",
       "      <td>sostoken ce n'est pas la première fois que ça ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     eng  \\\n",
       "33861           i make it a rule never to borrow money .   \n",
       "23961  certain details of the crime were not made pub...   \n",
       "27491       the two pieces were glued tightly together .   \n",
       "35947         i need you to help me move this bookcase .   \n",
       "17084      this isn't the first time this has happened .   \n",
       "\n",
       "                                                     fra  \n",
       "33861  sostoken j'ai érigé en règle de ne jamais empr...  \n",
       "23961  sostoken certains détails du crime n'ont pas é...  \n",
       "27491  sostoken les deux morceaux furent fermement co...  \n",
       "35947  sostoken j'ai besoin que tu m'aides à déplacer...  \n",
       "17084  sostoken ce n'est pas la première fois que ça ...  "
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sos_token = 'sostoken'\n",
    "eos_token = 'eostoken'\n",
    "lines.fra = lines.fra.apply(lambda x : 'sostoken '+ x + ' eostoken')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b738429",
   "metadata": {},
   "source": [
    "3. 띄어쓰기 단위로 토큰화를 수행하세요.\n",
    "\n",
    "띄어쓰기 단위로 토큰화를 수행해서 단어를 분리하는 작업을 해주세요. 기계는 이렇게 분리된 토큰들을 각각 하나의 단어로 인식할 수 있게 됩니다.\n",
    "\n",
    "> 토큰화 전 : 'he is a good boy !'  \n",
    "> 토큰화 후 : ['he', 'is', 'a', 'good', 'boy', '!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "9528ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize_by_word(sentence: str) -> list:\n",
    "    return sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "bd9ed20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Tokenize_by_word('he is a good boy !') == ['he', 'is', 'a', 'good', 'boy', '!'], 'word tokenizer error.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "2e90e63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize_by_word_for(df: pd.DataFrame) -> (list, list): # english, french\n",
    "    encode_sentences = []\n",
    "    decode_sentences = []\n",
    "    for index, row in df.iterrows():\n",
    "        encode_tokens = Tokenize_by_word(row['eng'])\n",
    "        decode_tokens = Tokenize_by_word(row['fra'])\n",
    "        encode_sentences.append(encode_tokens)\n",
    "        decode_sentences.append(decode_tokens)\n",
    "    return encode_sentences, decode_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "8c263bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_corpus, french_corpus = Tokenize_by_word_for(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479f5165",
   "metadata": {},
   "source": [
    "### Step 2. 디코더의 문장에 시작 토큰과 종료 토큰을 넣어주세요.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c915e2f5",
   "metadata": {},
   "source": [
    "글자 단위 번역기를 구현할 때와 마찬가지로 디코더의 입력 시퀀스 맨 앞에는 시작을 의미하는 토큰인 <sos>가 필요합니다. 그리고 교사 강요를 수행할 때, 디코더의 실제값이 되는 디코더의 레이블 시퀀스에는 종료를 의미하는 종료 토큰 <eos>가 필요합니다.\n",
    "예를 들어 번역 문장이 \"Courez!\" 였다고 한다면, Step 1을 거친 후에는 다음과 같은 결과를 얻습니다.\n",
    "\n",
    "> Step 1을 수행한 후 : ['courez', '!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "e7d460f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apparently',\n",
       " ',',\n",
       " 'the',\n",
       " 'murder',\n",
       " 'happened',\n",
       " 'in',\n",
       " 'a',\n",
       " 'locked',\n",
       " 'room',\n",
       " '.']"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79040331",
   "metadata": {},
   "source": [
    "이 문장에 대해서 각각 디코더의 입력 시퀀스와 레이블 시퀀스를 만들면 다음과 같습니다.\n",
    "\n",
    "> 입력 시퀀스 : ['< sos >', 'courez', '!']  \n",
    "> 레이블 시퀀스 : ['courez', '!', '< eos >']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "60ef86fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sostoken',\n",
       " 'apparemment',\n",
       " ',',\n",
       " 'le',\n",
       " 'meurtre',\n",
       " 'a',\n",
       " 'eu',\n",
       " 'lieu',\n",
       " 'en',\n",
       " 'chambre',\n",
       " 'close',\n",
       " '.',\n",
       " 'eostoken']"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97689d",
   "metadata": {},
   "source": [
    "참고로 Step 2가 반드시 Step 1이 끝난 후에 이루어질 필요는 없습니다!  \n",
    "Step 1을 수행하는 중간에 수행해도 상관없습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87643a91",
   "metadata": {},
   "source": [
    "### Step 3. 케라스의 토크나이저로 텍스트를 숫자로 바꿔보세요.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0313e55",
   "metadata": {},
   "source": [
    "딥러닝 모델은 텍스트가 아닌 숫자를 처리합니다. 케라스 토크나이저를 사용해서 각 단어를 고유한 정수로 바꿔보세요.\n",
    "케라스 토크나이저의 사용법은 아래의 링크에서 2. 케라스(Keras)의 텍스트 전처리에 설명되어 있습니다.\n",
    "\n",
    "[위키독스](https://wikidocs.net/31766)  \n",
    "\n",
    "위 링크의 가이드를 통해서 영어와 프랑스어에 대한 토크나이저를 각각 생성하고, tokenizer.texts_to_sequences()를 사용하여 모든 샘플에 대해서 정수 시퀀스로 변환해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "a100d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "d8070ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)        \n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "bf4db61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_text, english_tokenizer = tokenize(english_corpus)\n",
    "french_text, french_tokenizer = tokenize(french_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "652b4c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3254, 8, 2, 1196, 219, 9, 6, 1295, 170, 1],\n",
       " [20, 608, 653, 3, 216, 372, 2, 472, 1],\n",
       " [18, 11, 2, 623, 4, 253, 127, 98, 1]]"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "122e251d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 3008, 11, 9, 1376, 18, 143, 417, 23, 337, 10197, 3, 2],\n",
       " [1, 43, 648, 649, 437, 61, 6, 484, 3, 2],\n",
       " [1, 52, 9, 642, 7, 3683, 83, 17, 172, 3, 2]]"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_text[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be68621",
   "metadata": {},
   "source": [
    "####  인코딩 상태 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "02c67f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 문장수 36000\n",
      "프랑스어 문장수 36000\n"
     ]
    }
   ],
   "source": [
    "print('영어 문장수', len(english_text))\n",
    "print('프랑스어 문장수', len(french_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "454ba28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어수: 12246\n",
      "프랑스어 단어수: 19259\n"
     ]
    }
   ],
   "source": [
    "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
    "french_vocab_size = len(french_tokenizer.word_index) + 1\n",
    "print('영어 단어수:', english_vocab_size)\n",
    "print('프랑스어 단어수:', french_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "709e57c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 문장 최대 길이 : 51\n",
      "프랑스 문장 최대 길이 : 61\n"
     ]
    }
   ],
   "source": [
    "max_eng_len = max([len(s) for s in english_text])\n",
    "max_fre_len = max([len(s) for s in french_text])\n",
    "print('영어 문장 최대 길이 :', max_eng_len)\n",
    "print('프랑스 문장 최대 길이 :', max_fre_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6275d709",
   "metadata": {},
   "source": [
    "### Step 4. 임베딩 층(Embedding layer) 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0729869",
   "metadata": {},
   "source": [
    "이번에는 입력이 되는 각 단어를 임베딩 층을 사용하여 벡터화하겠습니다.\n",
    "임베딩 층을 사용하는 방법과 그 설명에 대해서는 아래의 링크의 1. 케라스 임베딩 층(Keras Embedding layer) 을 참고하세요.\n",
    "\n",
    "[위키독스](https://wikidocs.net/33793)\n",
    "\n",
    "실제 번역기 구현을 위해서 사용할 수 있는 인코더 코드의 예시는 다음과 같습니다. 이를 통해서 인코더와 디코더의 임베딩 층을 각각 구현해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "c70e3906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "84f6b5fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36000"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(french_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "a250c0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = english_text\n",
    "decoder_input = [sentence[:-1] for sentence in french_text]\n",
    "decoder_target = [sentence[1:] for sentence in french_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "c01112df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이타 크기(shape) : (36000, 51)\n",
      "프랑스어 입력 데이타 크기(shape) : (36000, 61)\n",
      "프랑스어 타겟 데이타 크기(shape) : (36000, 61)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fre_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fre_len, padding='post')\n",
    "print('영어 데이타 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력 데이타 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 타겟 데이타 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "83b6021b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 학습데이터의 크기(shape) : (30000, 51)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (30000, 61)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (30000, 61)\n",
      "\n",
      "영어 학습데이터의 크기(shape) : (3000, 51)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (3000, 61)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (3000, 61)\n",
      "\n",
      "영어 학습데이터의 크기(shape) : (3000, 51)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (3000, 61)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (3000, 61)\n"
     ]
    }
   ],
   "source": [
    "test_num = 3000\n",
    "val_num = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-6000]\n",
    "decoder_input_train = decoder_input[:-6000]\n",
    "decoder_target_train = decoder_target[:-6000]\n",
    "\n",
    "encoder_input_val = encoder_input[-6000:-3000]\n",
    "decoder_input_val = decoder_input[-6000:-3000]\n",
    "decoder_target_val = decoder_target[-6000:-3000]\n",
    "\n",
    "encoder_input_test = encoder_input[-3000:]\n",
    "decoder_input_test = decoder_input[-3000:]\n",
    "decoder_target_test = decoder_target[-3000:]\n",
    "\n",
    "\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input_train))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input_train))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target_train))\n",
    "print()\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input_val))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input_val))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target_val))\n",
    "print()\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input_test))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input_test))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "b58528f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3254    8    2 1196  219    9    6 1295  170    1    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a7d0d",
   "metadata": {},
   "source": [
    "#### 인코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "bb359ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텐서 정의 및 생성.\n",
    "eng_vocab_size = english_vocab_size\n",
    "fra_vocab_size = french_vocab_size\n",
    "\n",
    "# 인코더 시작\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb_layer =  Embedding(eng_vocab_size, embedding_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(units=256, return_state=True, dropout=0.2)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb_layer)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5606e9a2",
   "metadata": {},
   "source": [
    "#### 디코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "ad0164bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(fra_vocab_size, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs) \n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True, dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1011537f",
   "metadata": {},
   "source": [
    "#### 출력층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "577d1525",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6224f57b",
   "metadata": {},
   "source": [
    "### Step 5. 모델 구현하기\n",
    "---\n",
    "\n",
    "글자 단위 번역기에서 구현한 모델을 참고로 단어 단위 번역기의 모델을 완성시켜보세요! 이때는 label이 integer 값이므로 categorical entropy loss가 아닌 sparse categorical entropy loss를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "1c906dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "460e046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "565f7a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_27\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_38 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_39 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_16 (Embedding)        (None, None, 128)    1567488     input_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_17 (Embedding)        (None, None, 128)    2465152     input_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_16 (LSTM)                  [(None, 256), (None, 394240      embedding_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_17 (LSTM)                  [(None, None, 256),  394240      embedding_17[0][0]               \n",
      "                                                                 lstm_16[0][1]                    \n",
      "                                                                 lstm_16[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, None, 19259)  4949563     lstm_17[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,770,683\n",
      "Trainable params: 9,770,683\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba847e34",
   "metadata": {},
   "source": [
    "### 모델 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "ed80960e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 61)"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "244025a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "59/59 [==============================] - 32s 500ms/step - loss: 3.4110 - val_loss: 1.3408\n",
      "Epoch 2/30\n",
      "59/59 [==============================] - 29s 489ms/step - loss: 1.2953 - val_loss: 1.2929\n",
      "Epoch 3/30\n",
      "59/59 [==============================] - 29s 484ms/step - loss: 1.2482 - val_loss: 1.2332\n",
      "Epoch 4/30\n",
      "59/59 [==============================] - 29s 489ms/step - loss: 1.1806 - val_loss: 1.1739\n",
      "Epoch 5/30\n",
      "59/59 [==============================] - 29s 486ms/step - loss: 1.1348 - val_loss: 1.1351\n",
      "Epoch 6/30\n",
      "59/59 [==============================] - 29s 488ms/step - loss: 1.0987 - val_loss: 1.1016\n",
      "Epoch 7/30\n",
      "59/59 [==============================] - 29s 491ms/step - loss: 1.0650 - val_loss: 1.0746\n",
      "Epoch 8/30\n",
      "59/59 [==============================] - 29s 488ms/step - loss: 1.0375 - val_loss: 1.0511\n",
      "Epoch 9/30\n",
      "59/59 [==============================] - 29s 491ms/step - loss: 1.0128 - val_loss: 1.0311\n",
      "Epoch 10/30\n",
      "59/59 [==============================] - 29s 494ms/step - loss: 0.9912 - val_loss: 1.0149\n",
      "Epoch 11/30\n",
      "59/59 [==============================] - 29s 492ms/step - loss: 0.9730 - val_loss: 1.0000\n",
      "Epoch 12/30\n",
      "59/59 [==============================] - 29s 488ms/step - loss: 0.9556 - val_loss: 0.9873\n",
      "Epoch 13/30\n",
      "59/59 [==============================] - 29s 492ms/step - loss: 0.9394 - val_loss: 0.9757\n",
      "Epoch 14/30\n",
      "59/59 [==============================] - 29s 491ms/step - loss: 0.9240 - val_loss: 0.9631\n",
      "Epoch 15/30\n",
      "59/59 [==============================] - 29s 491ms/step - loss: 0.9072 - val_loss: 0.9503\n",
      "Epoch 16/30\n",
      "59/59 [==============================] - 29s 492ms/step - loss: 0.8915 - val_loss: 0.9389\n",
      "Epoch 17/30\n",
      "59/59 [==============================] - 29s 491ms/step - loss: 0.8757 - val_loss: 0.9273\n",
      "Epoch 18/30\n",
      "59/59 [==============================] - 29s 492ms/step - loss: 0.8604 - val_loss: 0.9163\n",
      "Epoch 19/30\n",
      "59/59 [==============================] - 29s 493ms/step - loss: 0.8453 - val_loss: 0.9048\n",
      "Epoch 20/30\n",
      "59/59 [==============================] - 29s 494ms/step - loss: 0.8307 - val_loss: 0.8947\n",
      "Epoch 21/30\n",
      "59/59 [==============================] - 29s 491ms/step - loss: 0.8162 - val_loss: 0.8823\n",
      "Epoch 22/30\n",
      "59/59 [==============================] - 29s 493ms/step - loss: 0.8013 - val_loss: 0.8728\n",
      "Epoch 23/30\n",
      "59/59 [==============================] - 29s 495ms/step - loss: 0.7871 - val_loss: 0.8628\n",
      "Epoch 24/30\n",
      "59/59 [==============================] - 29s 494ms/step - loss: 0.7741 - val_loss: 0.8550\n",
      "Epoch 25/30\n",
      "59/59 [==============================] - 29s 493ms/step - loss: 0.7613 - val_loss: 0.8469\n",
      "Epoch 26/30\n",
      "59/59 [==============================] - 29s 493ms/step - loss: 0.7486 - val_loss: 0.8402\n",
      "Epoch 27/30\n",
      "59/59 [==============================] - 29s 493ms/step - loss: 0.7372 - val_loss: 0.8330\n",
      "Epoch 28/30\n",
      "59/59 [==============================] - 29s 495ms/step - loss: 0.7256 - val_loss: 0.8274\n",
      "Epoch 29/30\n",
      "59/59 [==============================] - 29s 495ms/step - loss: 0.7145 - val_loss: 0.8203\n",
      "Epoch 30/30\n",
      "59/59 [==============================] - 29s 494ms/step - loss: 0.7039 - val_loss: 0.8137\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data = ([encoder_input_val, decoder_input_val], decoder_target_val), \n",
    "          batch_size=512, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "f48ab39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f1c46020310>"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(15,6))\n",
    "\n",
    "for k,v in history.history.items():\n",
    "    plt.plot(history.history[k], label=k)\n",
    "plt.ylim(0, 3)\n",
    "plt.title('RMS Prop, Batch Size 512')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780d3af2",
   "metadata": {},
   "source": [
    "### Step 6. 모델 평가하기\n",
    "---\n",
    "단어 단위 번역기를 이용하여 훈련 데이터의 샘플과 테스트 데이터의 샘플로 번역 문장을 만들어보고 정답 문장과 번역 문장을 비교해보세요. 이전 스텝들에서 우리가 공부했던 모델의 경우 글자 단위에서 구현된 번역기이며 현재 프로젝트를 진행할 때 사용하는 모델은 단어 단위에서 구현되는 번역기입니다.\n",
    "\n",
    "> Embedding layer가 추가되기 때문에 학습했던 내용 그대로 사용할 경우 shape에서 error가 발생합니다.\n",
    "> decode sentence를 구성할 때 고민해보세요!!\n",
    "\n",
    "고민하다 풀리지 않을 경우에는 하단 내용 참고해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "e7f95e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2idx = english_tokenizer.word_index\n",
    "fra2idx = french_tokenizer.word_index\n",
    "idx2eng = english_tokenizer.index_word\n",
    "idx2fra = french_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "76162655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 설계\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "hidden_size = 256\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# 디코더 설계 시작\n",
    "# 이전 시점의 상태를 보관할 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 훈련 때 사용했던 임베딩 층을 재사용\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "# 모든 시점에 대해서 단어 예측\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_outputs2)\n",
    "\n",
    "# 수정된 디코더\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9974ed93",
   "metadata": {},
   "source": [
    "### 번역기 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "be420219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <sos>에 해당하는 정수 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = fra2idx['sostoken']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 단어로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        if (sampled_token_index > 0):            \n",
    "            sampled_char = idx2fra[sampled_token_index]\n",
    "            # 현재 시점의 예측 단어를 예측 문장에 추가\n",
    "            decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # eos에 도달하거나 정해진 길이를 넘으면 중단.\n",
    "        if (sampled_token_index == 0 or\n",
    "            sampled_char == 'eostoken' or \n",
    "            len(decoded_sentence) > 30):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "bdee919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq_to_src(input_seq):\n",
    "    sentence = ''\n",
    "    for encoded_word in input_seq:\n",
    "        if(encoded_word != 0):\n",
    "            sentence = sentence + idx2eng[encoded_word] + ' '\n",
    "    return sentence\n",
    "\n",
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq_to_tar(input_seq):\n",
    "    sentence = ''\n",
    "    for encoded_word in input_seq:\n",
    "        if(encoded_word != 0 and encoded_word != fra2idx['sostoken'] and encoded_word != fra2idx['eostoken']):\n",
    "            sentence = sentence + idx2fra[encoded_word] + ' '\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "bc91542b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력문장 : he doesn't have the necessary skills for that job . \n",
      "정답문장 : il ne dispose pas des compétences nécessaires à cet emploi . \n",
      "번역문장 : tom a dit qu'il est un peu de \n",
      "--------------------------------------------------\n",
      "입력문장 : he carved a buddhist image out of wood . \n",
      "정답문장 : il a sculpté une image bouddhiste dans du bois . \n",
      "번역문장 : il a été de temps pour le\n",
      "--------------------------------------------------\n",
      "입력문장 : i asked her to marry me and she accepted . \n",
      "정답문장 : je lui demandai de m'épouser et elle accepta . \n",
      "번역문장 : je suis désolé de la maison ,\n",
      "--------------------------------------------------\n",
      "입력문장 : i am not used to drinking coffee without sugar . \n",
      "정답문장 : je ne suis pas habitué à boire du café sans sucre . \n",
      "번역문장 : je ne peux pas être en tra\n",
      "--------------------------------------------------\n",
      "입력문장 : i am going to study english this afternoon . \n",
      "정답문장 : je vais étudier l'anglais cet après-midi . \n",
      "번역문장 : je suis allé à boston pour la m\n",
      "--------------------------------------------------\n",
      "입력문장 : one thing you should know about me is that i'm a little overweight . \n",
      "정답문장 : une chose que tu devrais savoir à mon sujet , c'est que je suis un peu en surpoids . \n",
      "번역문장 : tom ne me faut pas que tu as\n",
      "--------------------------------------------------\n",
      "입력문장 : all the answers to this question were wrong . \n",
      "정답문장 : toutes les réponses à cette question étaient fausses . \n",
      "번역문장 : tom a été de temps pour le \n",
      "--------------------------------------------------\n",
      "입력문장 : his family dates back to the seventeenth century . \n",
      "정답문장 : sa famille remonte au dix-septième siècle . \n",
      "번역문장 : les enfants sont des états\n",
      "--------------------------------------------------\n",
      "입력문장 : i wonder if you have ever considered going to a specialist . \n",
      "정답문장 : je me demande si tu n'as jamais considéré te rendre chez un spécialiste . \n",
      "번역문장 : je pense que tu as eu un p\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [1, 10, 20, 30, 100, 200, 1000, 2000, 2500]:\n",
    "  input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "  print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n",
    "  print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n",
    "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
    "  print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0416b9",
   "metadata": {},
   "source": [
    "루브릭  \n",
    "아래의 기준을 바탕으로 프로젝트를 평가합니다.  \n",
    "평가문항\t상세기준\n",
    "\n",
    "1. 번역기 모델 학습에 필요한 텍스트 데이터 전처리가 잘 이루어졌다.  \n",
    "구두점, 대소문자, 띄어쓰기 등 번역기 모델에 요구되는 전처리가 정상적으로 진행되었다.  \n",
    "\n",
    "2. seq2seq 기반의 번역기 모델이 정상적으로 구동된다.  \n",
    "seq2seq 모델 훈련결과를 그래프로 출력해보고, validation loss그래프가 우하향하는 경향성을 보이며 학습이 진행됨이 확인되었다.\n",
    "\n",
    "3. 테스트 결과 의미가 통하는 수준의 번역문이 생성되었다.  \n",
    "테스트용 디코더 모델이 정상적으로 만들어졌으며, input(영어)와 output(프랑스어) 모두 한글로 번역해서 결과를 출력해보았고, 둘의 내용이 유사함을 확인하였다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a28e16",
   "metadata": {},
   "source": [
    "### 고찰\n",
    "1. 단어 단위 임베딩에 따른 인풋 벡터의 변화에 대한 이해가 어려웠다.\n",
    "2. 중복되는 문장이 있어 이를 제거하니  val loss가 약간 더 줄어드는 것처럼 보였다. 전처리와 데이타를 살펴보는 과정이 중요하다.\n",
    "3. 모델 학습에 소요되는 시간이 길어 여러가지 실험을 해보지 못한 점이 아쉬웠다.\n",
    "4. 모델 학습 시 오류가 발생해서 디버깅에 시간을 보냈다. 오류가 생긴 이유는 로스 함수의 선택이 잘못되었기 때문이었다. 입력 벡터의 차원이 맞지 않는다는 오류메세지가 나올 때도 원인은 엉뚱한 곳에서 나올 수 있으므로 기본적인 사항들을 모두 확인해야 한다는 점을 알았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51eb906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0204932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "392.796875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
